% !TeX spellcheck = en_GB

\begin{comment}
\begin{itemize}
	\item implementation in \gls{py} 3
	\item command line interface with subcommand as central entrypoint providing configuration and log management
	\item each process is run as separate processes to mitigate \gls{gil} limitations
	\item process has one task (agent simulator, collector, analyser module)
	\item modules/process pass messages mainly using \gls{amqp} and \gls{rabbitmq} as message broker
	\item reusing \gls{influxdb} and \gls{rabbitmq} by separating pipelines using a project name
\end{itemize}
\end{comment}

To validate, test, and proof the concept as described in Chapter~\ref{sec:concept}, it was implemented as prototype using \gls{py}3 and the \gls{sklearn} library for data analysis.
As the concept evolves around a message-passing architecture, the prototype follow this approach as well by using \gls{amqp} with \gls{rabbitmq} as message-broker. As central data storage the time-series database \gls{influxdb} is used, since it provides good querying capabilities and excellent integration with existing third-party solutions like \gls{grafana}, which will be used as monitoring and alerting solution.

This enables to run the various modules as separate processes on possibly different machines, as every message exchange is done via the message-broker and historical data can be queried directly form the database.
As a side effect the possible restrictions introduced by \gls{py}s \glsfirst{gil} are circumvented.
Even though each module is capsuled in an own process, all of them can be started using an unified \gls{cli}, which abstracts things like configuration and logging, as well as connection management for the \gls{amqp} message-broker and the database.
The implementation details of each module are described in-depth in the following sections.

\section{Basic Command Line Interface}
\label{sec:impl:cli}

\begin{itemize}
	\item central entry point for all operations
	\item consistent user experience -> less fiddling while debugging/developing
	\item centralises configuration and log setup
	\item can be easily made accessible via python setuptools
		\subitem if globally installed, callable via normal terminal command
	\item implemented using the \gls{lib-click} library version 6
\end{itemize}

\section{The Agent Simulator}
\label{sec:impl:agent}

\begin{itemize}
	\item simulates multiple agents based of one log containing telegram in raw \gls{baos} format. cf. Appendix~\todo{add log sample in appendix}
	\item log must be in chronological order
	\item utilizes own parser implementation \url{https://github.com/FreakyBytes/BaosKnxParser}
	\item different agents can be simulated by applying filter rules to log stream, defining, what each agent "can see"
	\item is supposed to replace actual agents during development
		\subitem repeatable data
		\subitem easy/fast setup
		\subitem log-time much faster than real-time
		\subitem load testing possible
	\item reads in individual pack
	\item filters according to agent filter rules
	\item updates agent-specific window data model
	\item if window length/timeout (cf. Section~\ref{sec:concept:agent}) is exceeded in log-time, windows are submitted to \gls{amqp} message broker (cf. Figure~\ref{fig:concept:architecture})
	\item runs until log is fully red, or maximum packets to parse are exceeded, or defined end timestamp is reached
\end{itemize}

\section{The Collector Module}
\label{sec:impl:collector}

\begin{itemize}
	\item central hub for relaying information
	\item receives agent windows via \gls{amqp} message broker (cf. Figure~\ref{fig:concept:architecture})
	\item subscribed to the agent topic using \gls{lib-pika} version 0.11
	\item incoming windows from agents are stored immediately in \gls{influxdb}
	\item every 4 seconds a callback \hint{(other word for callback?)} is called
	\item callback queries \gls{influxdb} for not relayed windows
		\subitem ordered latest first
		\subitem so a cumulating backlog does not affect the ability to show/process near-real-time metrics
		\subitem old windows will be processed once backlog decreases
	\item collector groups windows by timeslot
		\subitem so analysers receive a snapshot of the network
		\subitem allows them to analyse world-view without the need to compensate time-difference
\end{itemize}

\section{The Analyser Base Module}
\label{sec:impl:base}

\begin{itemize}
	\item non functional base class, unifying commonly used functions for analyser modules, including
		\subitem logging and config setup
		\subitem model persistence (loading and saving)
		\subitem acquiring training data from \gls{influxdb}
		\subitem common entrypoint functions for training and analyse modes
	\item training data is generated by first querying the \code{agent\_status} metric for the requested training period
	\item this metric is used to group the different agents by time slots, similar to what the collector (cf. Section~\ref{sec:impl:collector}) is doing
		\subitem fuzzy, allowing for up to 2 seconds between the capture timestamps of the different windows
		\subitem accommodate for clock drift of the agents and delays, jitter, etc of the transport
	\item once the grouping is finished the remaining metrics are queried for all windows in a time slots
	\item reduced RAM overhead during the querying and enables better (fuzzy) grouping, mitigating shortcoming in the \gls{influxdb} query language
\end{itemize}

\section{The Address Analyser Module}
\label{sec:impl:addr}

\begin{itemize}
	\item purpose is to detect the usage of prior unknown addresses
	\item (might) detect new (not malicious) devices
	\item stores all occurring source and destination addresses during the training period in a separately in a set
	\item set is stored to the disk using the standard \gls{py} \gls{json} serialiser
	\item during normal analytical operation the module checks if occurring addresses already occured in the training phase
	\item if not so counters are increased
	\item \todo{and list of unknown addresses is exported}
	\item exported metrics: \code{unknown\_src\_addr}, \code{unknown\_src\_telegrams}, \code{unknown\_dest\_addr}, \code{unknown\_dest\_telegrams}, \code{unknown\_addr}, \code{unknown\_telegrams}
\end{itemize}

\section{The Local Outlier Factor Analyser Module}
\label{sec:impl:lof}

\begin{itemize}
	\item purpose is to detect uncommon bus activity
	\item world view as well as agent based
	\item uses the \gls{lof} implementation of Scikit-learn \parencite{Pedregosa2011}
	%\item during training phase, grouped agent windows are extracted from \gls{influxdb} (effectively mimicking the collectors grouping algorithm)
	
	% feature vector
	\item for each window of each agent a \gls{fvect} is constructed from
		\subitem normalized seconds since the beginning of the year
		\subitem source addresses
		\subitem destination addresses
		\subitem priority distribution
		\subitem hop count distribution
		\subitem payload length distribution
		\subitem \gls{apci} usage distribution
	\item Both source and destination addresses are encoded as a vector of length 16
		\subitem each dimension in this vector represents the probability of occurrence of a address bit in the current window
		\subitem adaption of the hashing trick (cf. Section~\ref{sec:background:network:features:hashing})
		\subitem to reduce \gls{vect}, instead of using $2 \cdot 2^16 = 2 \cdot 65536 = 131072$ dimensions to model the \gls{pmf} of both address fields
		\subitem as a result only $2 \cdot 16 = 32$ dimensions to encode the addresses, without loosing information
	\item priority is encoded using an adoption of the OneHot encoding (cf. Section~\ref{sec:background:network:features:onehot}) to model the \gls{pmf} of the occurrence of priorities in the current window
		\subitem $4$ dimensions to encode the priorities \gls{pmf} (cf. Table~\ref{tab:background:bas:knx:proto:prio})
	\item hop count similarly encoded as priority, using an OneHot adaption
		\subitem $7$ dimension to encode the hop count \gls{pmf} (cf. Table~\ref{tab:background:bas:knx:proto:knx-standard}~and~\ref{tab:background:bas:knx:proto:ctrle})
	\item payload length uses a different adaption of the OneHot encoding (cf. Section~\ref{sec:background:network:features:onehot})
		\subitem the payload length (max $255$ Bytes) is divided into $10$ buckets to reduce the dimensions of the \gls{fvect}
		\subitem most analysed traffic has rather short payload
		\subitem therefore most telegrams will fit in the first or second bucket
		\subitem still clear distinction if packets with longer payload occur, just granularity gets lost
	\item \gls{apci} is model similar to priority and hop count using the same adaption of the OneHot encoding
		\subitem every of the possible \alert{37} \todo{(check if this is still valid, after the modifications of the baos lib)} \gls{apci} values is modelled as unique dimension
		\subitem overall representing the \gls{pmf} of occurrences per \gls{apci} values in the current window
		
	\item using this feature vector 2 models are trained per agent window
		\subitem one agent specific model
		\subitem one world model, which is trained using data from all agents (aka. the whole network)
	\item purpose of the agent model is to be able to detect traffic leakage
		\subitem i.e. traffic that is normal for one line (e.g. light switches and motion sensors) suddenly occurs in the line responsible for \gls{hvac} control
		\subitem agent model is highly trained and most probably highly sensitive, but can detect unusual behaviour which is normal for the network but not for the line
		\subitem world model is more general and not as sensitive, but cannot distinguish between purposes of lines
	\item analyser module stores 1 for being an outlier or 0 for not being an outlier in the \gls{influxdb} per window
		\subitem threshold is decided in analyser
		\subitem easier monitoring, but data get mangled doing this
		\subitem better might be to store the relative distance (output of \gls{lof}) as well
	
\end{itemize}

