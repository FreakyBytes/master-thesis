% !TeX spellcheck = en_GB

\begin{comment}
\begin{itemize}
	\item implementation in \gls{py} 3
	\item command line interface with subcommand as central entrypoint providing configuration and log management
	\item each process is run as separate processes to mitigate \gls{gil} limitations
	\item process has one task (agent simulator, collector, analyser module)
	\item modules/process pass messages mainly using \gls{amqp} and \gls{rabbitmq} as message broker
	\item reusing \gls{influxdb} and \gls{rabbitmq} by separating pipelines using a project name
\end{itemize}
\end{comment}

To validate, test, and proof the concept as described in Chapter~\ref{sec:concept}, it was implemented as prototype using \gls{py}3 and the \gls{sklearn} library for data analysis.
The working name for the prototype is \emph{BAS Observe}, or short \emph{BOb}.
As the concept evolves around a message-passing architecture, the prototype follow this approach as well by using \gls{amqp} with \gls{rabbitmq} as message-broker. As central data storage the time-series database \gls{influxdb} is used, since it provides good querying capabilities and excellent integration with existing third-party solutions like \gls{grafana}, which will be used as monitoring and alerting solution.

This enables to run the various modules as separate processes on possibly different machines, as every message exchange is done via the message-broker and historical data can be queried directly form the database.
As a side effect the possible restrictions introduced by \gls{py}s \glsfirst{gil} are circumvented.
Even though each module is capsuled in an own process, all of them can be started using an unified \gls{cli}, which abstracts things like configuration and logging, as well as connection management for the \gls{amqp} message-broker and the database.
For interacting with the \gls{rabbitmq} message-broker \gls{lib-pika} 0.11 is used. To query from and push values to the \gls{influxdb} time series database the official \gls{py} client side library is used.
The implementation details of each module are described in-depth in the following sections.

\section{The Command Line Interface}
\label{sec:impl:cli}

\begin{comment}
\begin{itemize}
	\item central entry point for all operations
	\item consistent user experience -> less fiddling while debugging/developing
	\item centralises configuration and log setup
	\item can be easily made accessible via python setuptools
		\subitem if globally installed, callable via normal terminal command
	\item implemented using the \gls{lib-click} library version 6
\end{itemize}
\end{comment}

The \glsfirst{cli} provides a central and consistent entry point for all operations and is implemented using \gls{lib-click}, which provides an fast way to define command line options and parameters with code annotations.
The \gls{cli} can be installed via \gls{py} setup tools and exports one command into the operating system's search path, called \code{bob}.
This exported main command does not provide any functionality itself, apart from taking basic configuration parameter like the database and message-broker host, log level, and a project identifier -- all of which are preset to sane defaults.
Rather the command exposes a series of subcommands.

The first being \code{bob collector}, which invokes the collector module and only requires a list of available agents additionally to the basic configuration and is further described in Section~\ref{sec:impl:collector}.
Besides the Collector an agent simulator can be invoked by \code{bob simulate}, which can simulate multiple Agents from the content of a \gls{knx} log file, as described in Section~\ref{sec:impl:agent}.
By only on replaying existing observations an controllable and repeatable test environment can be archived. An Agent implementation able to process live traffic was not considered necessary for the point of demonstrating and testing the proposed concept.

The Analyser modules, however, can be invoked by either two generic subcommands, \code{bob train} which starts a module in training mode or \code{bob analyse} which runs the module in the operational, analytical mode.
All training commands take tree additional parameters: a start and end time of the period to be used as training data and the path to a model file, where the training results can be stored. The module which is supposed to be trained can be selected by adding it as another subcommand, so would \code{bob train lof} start the training mode of the \gls{lof} Analyser.
Same applies for the operation mode, where, however, the start and end date can be omitted and only the path to a model file needs to be passed. So can the Address Analyser be called by \code{bob analyse addr}.

Additionally to starting modules, the \gls{cli} implementation also takes care of configuration management and logging.
Despite being not critical for demonstrating the performance of the implemented concept, a consistent, easy, and documented mechanism for running modules eases development and hopefully will increase the reuse-ability of the source code.
A feature that deliberately was left out were configuration files. For the purpose of showcasing the concept it did not seem necessary and comes with the risk of hiding configuration parameter out of sight, which possibly increases the risk of mistakes made just because a changed parameter was forgotten. By plainly relying on command line parameters for configuration, the configuration is always in plain sight, when invoking a command.

\todo{explain 'project' notion}

\section{The Collector Module}
\label{sec:impl:collector}

\begin{comment}
\begin{itemize}
	\item central hub for relaying information
	\item receives agent windows via \gls{amqp} message broker (cf. Figure~\ref{fig:concept:architecture})
	\item subscribed to the agent topic using \gls{lib-pika} version 0.11
	\item incoming windows from agents are stored immediately in \gls{influxdb}
	\item every 4 seconds a callback \hint{(other word for callback?)} is called
	\item callback queries \gls{influxdb} for not relayed windows
		\subitem ordered latest first
		\subitem so a cumulating backlog does not affect the ability to show/process near-real-time metrics
		\subitem old windows will be processed once backlog decreases
	\item collector groups windows by timeslot
		\subitem so analysers receive a snapshot of the network
		\subitem allows them to analyse world-view without the need to compensate time-difference
\end{itemize}
\end{comment}

The central hub for relaying and transforming information is implemented in the Collector module.
It receives \gls{json}-encoded windows with statistical data from Agents by subscribing to an \gls{amqp} topic on the message-broker. Each incoming window is parsed, checked for format errors, and then immediately converted into a format suitable for \gls{influxdb} and pushed to the database. Only then the processing of the window is acknowledged to the message-broker, which ensures that no windows is dropped in cases Collector implementation crashes. This is archived by relying on retry functionality of message queues in the \gls{rabbitmq} broker and only requesting one message at a time.

Additionally, the Collector relays all windows belonging to one time slot to the Analyser modules, once one window from each Agent is written to the database.
For this a callback is invoked every $4$ seconds, which queries the $50$ latest, not relayed windows from the database. Whereby only the management measurement is queried, to reduce the size of data that needs to be handled.
These windows are then sorted into a hash-map of time slots, where the key is the averaged timestamp of all windows in this slot. The idea is to allow a certain degree (up to $2$ seconds) of clock drift in the Agents. 
So, if a window is added to the hash-map, a loop iterates over all existing keys and returns the key closest to the timestamp of the window, but with a maximum delta of $2$ seconds.
The window is then added to that timeslot and the key is recalculated as the average of all window timestamps in this slot, including the newly added one.
If no timeslot within $2$ seconds around the window exists, a new one will be created with the window's timestamp as key.

Once all queried windows are sorted into the hash-map, the Collector checks if a time slot contains windows from all Agents, by validating against a configurable list of Agent-ids.
When the time slot is complete, the windows are enriched by querying all remaining measurements from the database and then relayed to the Analyser modules via an \gls{amqp} message exchange, which distributes it into multiple message queues.
Only if the windows were successfully passed into to the message-broker, they are marked as relayed in the database. This ensures, that every window is processed. However, this way can not prevent time slots to send repeatedly to the Analyser modules, if the Collector crashes right before marking them as relayed in the database. In contrast to receiving windows form Agents and initially writing them to the database, this does no harm, since the analytical algorithms are deterministic and therefore produce the same results for the same data input. Therefore only computational resources would be wasted, as \gls{influxdb} automatically filters redundant time series entries.

In the case one Agent's window is never received by the Collector, it waits a configurable timeout of about 60 seconds before relaying a window anyway. This ensures that a time slot is analyses even when an Agent fails, regardless of the failure mode. As this is an anomaly, which can be easily queried in the monitoring and alerting system, it is also detected there and consequently not handled in the Collector apart from a warning in the log.

\section{The Agent Simulator}
\label{sec:impl:agent}

\begin{itemize}
	\item simulates multiple agents based of one log containing telegram in raw \gls{baos} format. cf. Appendix~\todo{add log sample in appendix}
	\item log must be in chronological order
	\item utilizes own parser implementation \url{https://github.com/FreakyBytes/BaosKnxParser}
	\item different agents can be simulated by applying filter rules to log stream, defining, what each agent "can see"
	\item is supposed to replace actual agents during development
	\subitem repeatable data
	\subitem easy/fast setup
	\subitem log-time much faster than real-time
	\subitem load testing possible
	\item reads in individual pack
	\item filters according to agent filter rules
	\item updates agent-specific window data model
	\item if window length/timeout (cf. Section~\ref{sec:concept:agent}) is exceeded in log-time, windows are submitted to \gls{amqp} message broker (cf. Figure~\ref{fig:concept:architecture})
	\item runs until log is fully red, or maximum packets to parse are exceeded, or defined end timestamp is reached
\end{itemize}

The original idea and concept describes multiple Agents, possibly in custom hardware, listening to lines, aggregating traffic, and sending statistical windows to the collector.
In the sense of developing, testing, and benchmarking the concept and prototype implementation this does not seem appealing. Primarily, because by using actual \gls{knx} hardware in a test network would unnecessarily increase complexity and introduce more error sources, since an actual operational Agent has to be developed.
Further, the test environment can only kept stable with enormous effort, due to the fact, that sensors like buttons or motion detectors would need to be triggered manually in an consistent way. This alone makes it impossible to design repeatable tests and benchmarks.
Also there is no guarantee, that a test network will resemble an actual \gls{knx} network.

All this led to the decision to not implement a real operation Agent, but rather an Agent Simulator, which can emulate the behaviour of multiple Agents.
For this it reads a tab separated \gls{knx} dump containing a timestamp and raw \gls{knx} telegrams, encoded in the \gls{baos} format.
These dump files were easy to produce out of a database, containing a significant large recording of the \gls{knx} traffic of one line if the computer science building.

Since the module used to capture the line traffic, exports not normal \gls{knx} telegrams, but rather a proprietary format resembling an extended data telegram (cf. Section~\ref{sec:background:bas:knx:proto:data}), a custom parser is required and is integrated into the prototype implementation as \gls{py} library.
It takes the tab separated log \gls{knx} dump as input, assuming it is in chronological order, parses the packets and updates multiple statistical windows, one for each simulated Agent. Whereby the traffic the different simulated Agents can is distinguished by address filter rules, which work similar to \gls{ip} subnets masks. 
After the window length exceeded in log time, they are encode in \gls{json} and send to the collector via the message-broker.
... baos format... parser... etc...

\section{The Analyser Base Module}
\label{sec:impl:base}

\begin{itemize}
	\item non functional base class, unifying commonly used functions for analyser modules, including
		\subitem logging and config setup
		\subitem model persistence (loading and saving)
		\subitem acquiring training data from \gls{influxdb}
		\subitem common entrypoint functions for training and analyse modes
	\item training data is generated by first querying the \code{agent\_status} metric for the requested training period
	\item this metric is used to group the different agents by time slots, similar to what the collector (cf. Section~\ref{sec:impl:collector}) is doing
		\subitem fuzzy, allowing for up to 2 seconds between the capture timestamps of the different windows
		\subitem accommodate for clock drift of the agents and delays, jitter, etc of the transport
	\item once the grouping is finished the remaining metrics are queried for all windows in a time slots
	\item reduced RAM overhead during the querying and enables better (fuzzy) grouping, mitigating shortcoming in the \gls{influxdb} query language
\end{itemize}

\section{The Address Analyser Module}
\label{sec:impl:addr}

\begin{itemize}
	\item purpose is to detect the usage of prior unknown addresses
	\item (might) detect new (not malicious) devices
	\item stores all occurring source and destination addresses during the training period in a separately in a set
	\item set is stored to the disk using the standard \gls{py} \gls{json} serialiser
	\item during normal analytical operation the module checks if occurring addresses already occured in the training phase
	\item if not so counters are increased
	\item \todo{and list of unknown addresses is exported}
	\item exported metrics: \code{unknown\_src\_addr}, \code{unknown\_src\_telegrams}, \code{unknown\_dest\_addr}, \code{unknown\_dest\_telegrams}, \code{unknown\_addr}, \code{unknown\_telegrams}
\end{itemize}

\section{The Local Outlier Factor Analyser Module}
\label{sec:impl:lof}

\begin{itemize}
	\item purpose is to detect uncommon bus activity
	\item world view as well as agent based
	\item uses the \gls{lof} implementation of Scikit-learn \parencite{Pedregosa2011}
	%\item during training phase, grouped agent windows are extracted from \gls{influxdb} (effectively mimicking the collectors grouping algorithm)
	
	% feature vector
	\item for each window of each agent a \gls{fvect} is constructed from
		\subitem normalized seconds since the beginning of the year
		\subitem source addresses
		\subitem destination addresses
		\subitem priority distribution
		\subitem hop count distribution
		\subitem payload length distribution
		\subitem \gls{apci} usage distribution
	\item Both source and destination addresses are encoded as a vector of length 16
		\subitem each dimension in this vector represents the probability of occurrence of a address bit in the current window
		\subitem adaption of the hashing trick (cf. Section~\ref{sec:background:network:features:hashing})
		\subitem to reduce \gls{vect}, instead of using $2 \cdot 2^16 = 2 \cdot 65536 = 131072$ dimensions to model the \gls{pmf} of both address fields
		\subitem as a result only $2 \cdot 16 = 32$ dimensions to encode the addresses, without loosing information
	\item priority is encoded using an adoption of the OneHot encoding (cf. Section~\ref{sec:background:network:features:onehot}) to model the \gls{pmf} of the occurrence of priorities in the current window
		\subitem $4$ dimensions to encode the priorities \gls{pmf} (cf. Table~\ref{tab:background:bas:knx:proto:prio})
	\item hop count similarly encoded as priority, using an OneHot adaption
		\subitem $7$ dimension to encode the hop count \gls{pmf} (cf. Table~\ref{tab:background:bas:knx:proto:knx-standard}~and~\ref{tab:background:bas:knx:proto:ctrle})
	\item payload length uses a different adaption of the OneHot encoding (cf. Section~\ref{sec:background:network:features:onehot})
		\subitem the payload length (max $255$ Bytes) is divided into $10$ buckets to reduce the dimensions of the \gls{fvect}
		\subitem most analysed traffic has rather short payload
		\subitem therefore most telegrams will fit in the first or second bucket
		\subitem still clear distinction if packets with longer payload occur, just granularity gets lost
	\item \gls{apci} is model similar to priority and hop count using the same adaption of the OneHot encoding
		\subitem every of the possible \alert{37} \todo{(check if this is still valid, after the modifications of the baos lib)} \gls{apci} values is modelled as unique dimension
		\subitem overall representing the \gls{pmf} of occurrences per \gls{apci} values in the current window
		
	\item using this feature vector 2 models are trained per agent window
		\subitem one agent specific model
		\subitem one world model, which is trained using data from all agents (aka. the whole network)
	\item purpose of the agent model is to be able to detect traffic leakage
		\subitem i.e. traffic that is normal for one line (e.g. light switches and motion sensors) suddenly occurs in the line responsible for \gls{hvac} control
		\subitem agent model is highly trained and most probably highly sensitive, but can detect unusual behaviour which is normal for the network but not for the line
		\subitem world model is more general and not as sensitive, but cannot distinguish between purposes of lines
	\item analyser module stores 1 for being an outlier or 0 for not being an outlier in the \gls{influxdb} per window
		\subitem threshold is decided in analyser
		\subitem easier monitoring, but data get mangled doing this
		\subitem better might be to store the relative distance (output of \gls{lof}) as well
	
\end{itemize}

