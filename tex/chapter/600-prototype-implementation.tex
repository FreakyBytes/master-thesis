% !TeX spellcheck = en_GB

\begin{comment}
\begin{itemize}
	\item implementation in \gls{py} 3
	\item command line interface with subcommand as central entrypoint providing configuration and log management
	\item each process is run as separate processes to mitigate \gls{gil} limitations
	\item process has one task (agent simulator, collector, analyser module)
	\item modules/process pass messages mainly using \gls{amqp} and \gls{rabbitmq} as message broker
	\item reusing \gls{influxdb} and \gls{rabbitmq} by separating pipelines using a project name
\end{itemize}
\end{comment}

To validate, test, and proof the concept as described in Chapter~\ref{sec:concept}, it was implemented as prototype using \gls{py}3 and the \gls{sklearn} library for data analysis.
The working name for the prototype is \emph{BAS Observe}, or short \emph{BOb}.
As the concept evolves around a message-passing architecture, the prototype follow this approach as well by using \gls{amqp} with \gls{rabbitmq} as message-broker. As central data storage the time-series database \gls{influxdb} is used, since it provides good querying capabilities and excellent integration with existing third-party solutions like \gls{grafana}, which will be used as monitoring and alerting solution.

This enables to run the various modules as separate processes on possibly different machines, as every message exchange is done via the message-broker and historical data can be queried directly form the database.
As a side effect the possible restrictions introduced by \gls{py}'s \glsfirst{gil} are circumvented.
Even though each module is capsuled in an own process, all of them can be started using an unified \gls{cli}, which abstracts things like configuration and logging, as well as connection management for the \gls{amqp} message-broker and the database.
For interacting with the \gls{rabbitmq} message-broker \gls{lib-pika} 0.11 is used. To query from and push values to the \gls{influxdb} time series database the official \gls{py} client side library is used.
The implementation details of each module are described in-depth in the following sections.

\section{The Command Line Interface}
\label{sec:impl:cli}

\begin{comment}
\begin{itemize}
	\item central entry point for all operations
	\item consistent user experience -> less fiddling while debugging/developing
	\item centralises configuration and log setup
	\item can be easily made accessible via python setuptools
		\subitem if globally installed, callable via normal terminal command
	\item implemented using the \gls{lib-click} library version 6
\end{itemize}
\end{comment}

The \glsfirst{cli} provides a central and consistent entry point for all operations and is implemented using \gls{lib-click}, which provides an fast way to define command line options and parameters with code annotations.
The \gls{cli} can be installed via \gls{py} setup tools and exports one command into the operating system's search path, called \code{bob}.
This exported main command does not provide any functionality itself, apart from taking basic configuration parameter like the database and message-broker host, log level, and a project identifier -- all of which are preset to sane defaults.
Rather the command exposes a series of subcommands.

The first being \code{bob collector}, which invokes the collector module and only requires a list of available agents additionally to the basic configuration and is further described in Section~\ref{sec:impl:collector}.
Besides the Collector an agent simulator can be invoked by \code{bob simulate}, which can simulate multiple Agents from the content of a \gls{knx} log file, as described in Section~\ref{sec:impl:agent}.
By only on replaying existing observations an controllable and repeatable test environment can be archived. An Agent implementation able to process live traffic was not considered necessary for the point of demonstrating and testing the proposed concept.

The Analyser modules, however, can be invoked by either two generic subcommands, \code{bob train} which starts a module in training mode or \code{bob analyse} which runs the module in the operational, analytical mode.
All training commands take tree additional parameters: a start and end time of the period to be used as training data and the path to a model file, where the training results can be stored. The module which is supposed to be trained can be selected by adding it as another subcommand, so would \code{bob train lof} start the training mode of the \gls{lof} Analyser.
Same applies for the operation mode, where, however, the start and end date can be omitted and only the path to a model file needs to be passed. So can the Address Analyser be called by \code{bob analyse addr}.

Additionally to starting modules, the \gls{cli} implementation also takes care of configuration management and logging.
Despite being not critical for demonstrating the performance of the implemented concept, a consistent, easy, and documented mechanism for running modules eases development and hopefully will increase the reuse-ability of the source code.
A feature that deliberately was left out were configuration files. For the purpose of showcasing the concept it did not seem necessary and comes with the risk of hiding configuration parameter out of sight, which possibly increases the risk of mistakes made just because a changed parameter was forgotten. By plainly relying on command line parameters for configuration, the configuration is always in plain sight, when invoking a command.

\todo{explain 'project' notion}

\section{The Collector Module}
\label{sec:impl:collector}

\begin{comment}
\begin{itemize}
	\item central hub for relaying information
	\item receives agent windows via \gls{amqp} message broker (cf. Figure~\ref{fig:concept:architecture})
	\item subscribed to the agent topic using \gls{lib-pika} version 0.11
	\item incoming windows from agents are stored immediately in \gls{influxdb}
	\item every 4 seconds a callback \hint{(other word for callback?)} is called
	\item callback queries \gls{influxdb} for not relayed windows
		\subitem ordered latest first
		\subitem so a cumulating backlog does not affect the ability to show/process near-real-time metrics
		\subitem old windows will be processed once backlog decreases
	\item collector groups windows by timeslot
		\subitem so analysers receive a snapshot of the network
		\subitem allows them to analyse world-view without the need to compensate time-difference
\end{itemize}
\end{comment}

The central hub for relaying and transforming information is implemented in the Collector module.
It receives \gls{json}-encoded windows with statistical data from Agents by subscribing to an \gls{amqp} topic on the message-broker. Each incoming window is parsed, checked for format errors, and then immediately converted into a format suitable for \gls{influxdb} and pushed to the database. Only then the processing of the window is acknowledged to the message-broker, which ensures that no windows is dropped in cases Collector implementation crashes. This is archived by relying on retry functionality of message queues in the \gls{rabbitmq} broker and only requesting one message at a time.

Additionally, the Collector relays all windows belonging to one time slot to the Analyser modules, once one window from each Agent is written to the database.
For this a callback is invoked every $4$ seconds, which queries the $50$ latest, not relayed windows from the database. Whereby only the management measurement is queried containing the window length, star and end time, and relayed status, to reduce the size of data that needs to be handled.
These windows are then sorted into a hash-map of time slots, where the key is the averaged timestamp of all windows in this slot. The idea is to allow a certain degree (up to $2$ seconds) of clock drift in the Agents. 
So, if a window is added to the hash-map, a loop iterates over all existing keys and returns the key closest to the timestamp of the window, but with a maximum delta of $2$ seconds.
The window is then added to that timeslot and the key is recalculated as the average of all window timestamps in this slot, including the newly added one.
If no timeslot within $2$ seconds around the window exists, a new one will be created with the window's timestamp as key.

Once all queried windows are sorted into the hash-map, the Collector checks if a time slot contains windows from all Agents, by validating against a configurable list of Agent-ids.
When the time slot is complete, the windows are enriched by querying all remaining measurements from the database and then relayed to the Analyser modules via an \gls{amqp} message exchange, which distributes it into multiple message queues.
Only if the windows were successfully passed into to the message-broker, they are marked as relayed in the database. This ensures, that every window is processed. However, this way can not prevent time slots to send repeatedly to the Analyser modules, if the Collector crashes right before marking them as relayed in the database. In contrast to receiving windows form Agents and initially writing them to the database, this does no harm, since the analytical algorithms are deterministic and therefore produce the same results for the same data input. Therefore only computational resources would be wasted, as \gls{influxdb} automatically filters redundant time series entries.

In the case one Agent's window is never received by the Collector, it waits a configurable timeout of about 60 seconds before relaying a window anyway. This ensures that a time slot is analyses even when an Agent fails, regardless of the failure mode. As this is an anomaly, which can be easily queried in the monitoring and alerting system, it is also detected there and consequently not handled in the Collector apart from a warning in the log.

\section{The Agent Simulator}
\label{sec:impl:agent}

\begin{comment}
\begin{itemize}
	\item simulates multiple agents based of one log containing telegram in raw \gls{baos} format. cf. Appendix~\todo{add log sample in appendix}
	\item log must be in chronological order
	\item utilizes own parser implementation \url{https://github.com/FreakyBytes/BaosKnxParser}
	\item different agents can be simulated by applying filter rules to log stream, defining, what each agent "can see"
	\item is supposed to replace actual agents during development
		\subitem repeatable data
		\subitem easy/fast setup
		\subitem log-time much faster than real-time
		\subitem load testing possible
	\item reads in individual pack
	\item filters according to agent filter rules
	\item updates agent-specific window data model
	\item if window length/timeout (cf. Section~\ref{sec:concept:agent}) is exceeded in log-time, windows are submitted to \gls{amqp} message broker (cf. Figure~\ref{fig:concept:architecture})
	\item runs until log is fully red, or maximum packets to parse are exceeded, or defined end timestamp is reached
\end{itemize}
\end{comment}

The original idea and concept describes multiple Agents, possibly in custom hardware, listening to lines, aggregating traffic, and sending statistical windows to the collector.
In the sense of developing, testing, and benchmarking the concept and prototype implementation this does not seem appealing. Primarily, because by using actual \gls{knx} hardware in a test network would unnecessarily increase complexity and introduce more error sources, since an actual operational Agent has to be developed.
Further, the test environment can only kept stable with enormous effort, due to the fact, that sensors like buttons or motion detectors would need to be triggered manually in an consistent way. This alone makes it impossible to design repeatable tests and benchmarks.
Also there is no guarantee, that a test network will resemble an actual \gls{knx} network.

All this led to the decision to not implement a real operation Agent, but rather an Agent Simulator, which can emulate the behaviour of multiple Agents.
For this it reads a tab separated \gls{knx} dump containing a timestamp and raw \gls{knx} telegrams, encoded in the \gls{baos} format.
These dump files were easy to produce out of a database, containing a significant large recording of the \gls{knx} traffic of one line if the computer science building.

Since the module used to capture the actual line traffic initially, exports not normal \gls{knx} telegrams, but rather a proprietary format resembling an extended data telegram called \gls{baos}, the \gls{knx} dump is also encoded in this format.
Therefore a custom parser is required and was integrated into the prototype implementation as \gls{py} library.
The parser takes the tab separated log \gls{knx} dump as input, assuming chronological order, parses the packets and updates multiple statistical windows, one for each simulated Agent. Whereby the traffic the different simulated Agents can is distinguished by address filter rules, which work similar to \gls{ip} subnets masks. 
After the window length exceeded in log time, they are encode in \gls{json} and send to the collector via the message-broker.
The statistical window is not based on the feature vector (see Section~\ref{sec:concept:anal:feature-vector}). Instead the appearance of certain features are count, e.g. the source address \code{1.1.15} appeared $15$ times.
Every further processing, like normalisation and vectorising is done by the Collector. 

\section{The Analyser Base Module}
\label{sec:impl:base}

\begin{comment}
\begin{itemize}
	\item non functional base class, unifying commonly used functions for analyser modules, including
		\subitem logging and config setup
		\subitem model persistence (loading and saving)
		\subitem acquiring training data from \gls{influxdb}
		\subitem common entrypoint functions for training and analyse modes
	\item training data is generated by first querying the \code{agent\_status} metric for the requested training period
	\item this metric is used to group the different agents by time slots, similar to what the collector (cf. Section~\ref{sec:impl:collector}) is doing
		\subitem fuzzy, allowing for up to 2 seconds between the capture timestamps of the different windows
		\subitem accommodate for clock drift of the agents and delays, jitter, etc of the transport
	\item once the grouping is finished the remaining metrics are queried for all windows in a time slots
	\item reduced RAM overhead during the querying and enables better (fuzzy) grouping, mitigating shortcoming in the \gls{influxdb} query language
\end{itemize}
\end{comment}

Once the statistical windows are gathered by the Agent simulator and aggregated by the Collector they are passed on the several Analyser modules.
Since the handling of persist models, receiving of statistical windows, and querying of training data are tasks common to all Analyser and contain a significant amount of boilerplate code it was abstracted in a base class.
This base class also exposes a set of methods to invoke any Analyser via an standardised interface.

Besides a common interface and initialisation code providing access to configuration and logging utilities, the base class implements a methods to acquire training data and handle persistence of trained models.
The training data is queried from the \gls{influxdb} by resembling the behaviour of the Collector when grouping Agent windows together.
Similar to the Collector only the management measurement is queried initially, which is used to sort all windows into time slots, but allowing for up to two seconds of jitter within the windows' timestamps to accommodate for clock drift or similar effects.
Once a window is assigned to a time slot the remaining measurements are queried and stored in memory. It was found during test runs, that using multiple precise queries instead of one large one significantly improves the performance, resource consumption, and response rate of the \gls{influxdb}.
When the requested time span of training data is gathered it is passed back to actual training function, implemented in the specific Analyser modules.

Another important task handled within the base class is the persistence of models, meaning saving and loading trained base-line models to disk.
In case the base-line model only consists of standard \gls{py} object, these are simply serialised into \gls{json} and stored in the file system, like it is done for the address Analyser described in Section~\ref{sec:impl:addr}.
However, if the base-line model is trained using \gls{sklearn}, it can often not be stored as a \gls{json} representation. Therefore the in \gls{sklearn} integrated binary serialiser is used. Since one base-line model consists actually of many sub-models to account for world-view and Agent specific models (cf. Section~\ref{sec:concept:anal:lof}), multiple files are written to the file system. To keep track of all those sub-models, the mapping between the Agent ID or the world-view model and the file name for their binary representation are stored in an central \gls{json} file. This \gls{json} file is used to point to for configuration purposes, so the user does not have to supply the Agent ID to file mapping by hand.

\section{The Address Analyser Module}
\label{sec:impl:addr}

\begin{comment}
\begin{itemize}
	\item purpose is to detect the usage of prior unknown addresses
	\item (might) detect new (not malicious) devices
	\item stores all occurring source and destination addresses during the training period in a separately in a set
	\item set is stored to the disk using the standard \gls{py} \gls{json} serialiser
	\item during normal analytical operation the module checks if occurring addresses already occured in the training phase
	\item if not so counters are increased
	\item \todo{and list of unknown addresses is exported}
	\item exported metrics: \code{unknown\_src\_addr}, \code{unknown\_src\_telegrams}, \code{unknown\_dest\_addr}, \code{unknown\_dest\_telegrams}, \code{unknown\_addr}, \code{unknown\_telegrams}
\end{itemize}
\end{comment}

The simplest implementation build upon the Analyser Base module is the Address Analyser.
As specified in the concept (see Section~\ref{sec:concept:anal:addr}) its purpose is to detect the usage of prior unknown addresses.

For this all addresses, which occur during the defined training period, are stored as a \gls{py} \code{set}, separated by Agent ID and whether it was discovered as source or destination address. 
The \code{set} data type ensures that first, every address is unique within one set -- eliminating redundancies, and secondly that it can checked if address is stored within a set very resource efficiently.
Once the training phase is complete, all sets (two per Agent) are stored using the methods the base class provides.

During the operational phase, the model containing all sets is loaded. Following all addresses in incoming windows are compared to their respective address set.
If an unknown address is discovered an counter will be increased. After a window is processed measurement containing these counters per Agent is pushed to the \gls{influxdb}, which contains:

\begin{itemize}
	\item amount of unique unknown source addresses
	\item amount of unique unknown destination addresses
	\item number of telegrams using an unknown source address
	\item number of telegrams using an unknown destination address
	\item sum of unique unknown addresses
	\item sum of telegrams using an unknown address
\end{itemize}

Additionally, a log warning is produced every time an unknown address is discovered. The warning includes the address, amount of telegrams, and direction (source or destination address).
For an implementation beyond the prototype status, an alert send to the administrator containing these information would be desirable.
However, in an testing environment log message seem sufficient.

\section{Converting a Window into a Feature Vector}
\label{sec:impl:vectoriser}

For more sophisticated analytical modules using machine learning algorithms, the input data needs to encoded as feature vector.
Within the proposed concept this means, that a statistical window has to be transformed into a numerical vector representation.
The basics principle of this is described in Section~\ref{sec:concept:anal:feature-vector}, whereby this section focusses on the details how each feature is encoded.
It is to note, that a window contains discrete or categorical statistical data describing a set of events, instead of just a single event, meaning the window produced by an Agent contains a number of occurrences over the period of the window.
The feature vector, however, will contain a normalised excerpt of the events' features, whereby each feature is encoded as one or more dimensions.
Mentioned excerpt of fields contains a set of low level, application independent, and easy to measure fields: 

\begin{itemize}
	\item seconds of the week
	\item source address
	\item destination address
	\item priority
	\item hop count
	\item payload length
	\item \gls{apci}
\end{itemize}

The first feature to encode is the time of the window within a defined period. For this prototype implementation this seasonal period was set to one week. Consequently the dimensions contains the seconds since the last Monday, normalised to the length of a 7 day week in seconds.

Both addresses fields are encoded with 16 dimensions each. Every dimension represents the frequency of occurrence of one bit, normalised against the overall number of telegrams observed in the current window.
Example given the numerical address $2$ would appear twice and the numerical address $1$ three times within a window. Given these are the only addresses in this window and the length of addresses would be 2 bit, the final feature vector dimensions for this address would be calculated as follow:
\[
\dfrac{2 \cdot \begin{pmatrix}1 & 0\end{pmatrix} + 3 \cdot \begin{pmatrix}0 & 1\end{pmatrix}}{2 + 3} = \begin{pmatrix}0.4 & 0.6\end{pmatrix}
\]
The result is an vector encoding the probability of each bit to occur within this specific window.
Goal was to reduce the amount of dimension, which would have been necessary when the entirety of possible addresses is mapped into the feature space -- each address using one dimension to encode the possibility of occurrence.
For the two address types, this would result in $2 \cdot 2^{16} = 2 \cdot 65536 = 131072$ dimensions. By applying this adapted form of the hashing trick (cf. Section~\ref{sec:background:network:features:hashing}) it was possible to reduce it to $2 \cdot 16 = 32$ dimensions.

In contrast to the addresses the priority of \gls{knx} telegrams has only four possible values. (see Table~\ref{tab:background:bas:knx:proto:prio}) Therefore the frequency of appearance of these four values can be directly mapped into feature vector as four dimensions. Similar to the addresses the frequency of each distinct priority is normalised against the overall number of telegrams observed in the current window.
The result are four dimensions, each of which represents the probability of telegram having the respective priority in the context of the current window, which can be compared to the OneHot encoding as described in Section~\ref{sec:background:network:features:onehot}.
The hop count is encoded equally, just with seven dimensions instead of four, as the maximum hop count for \gls{knx} telegrams is seven.

Initially the same encoding, used for priority and hop count, was also applied to the payload length. However, since the maximum payload length for a \gls{knx} telegram is $255$ Bytes, which would result in $255$ dimensions in the feature vector.
By encoding a feature using so many dimensions, a high granularity is introduced. For a feature like the payload length this granularity comes with a risk of over fitting the base-line model, as a small shift in the distribution of lengths could place the window in an significant different spot within the feature space.
To minimize this risk the range of the payload length is divided into ten equally sized buckets. These buckets are then encoded into the feature vector the same way as priority and hop count.
Each bucket is assigned a value representing the number of observed telegrams having a payload length which is within the range of the bucket. These values are then normalised against the number of overall observed telegrams for the period of the window.
The result is vector with the size 10 representing the distribution of payload length with an reduced granularity.

Finally, the \gls{apci}, which represents the application of the telegram, is again encoded using the same adaption of the OneHot encoding as used for the as for priority and hop count. This results in vector which one dimension for every possible \gls{apci}, whereby each dimension represents the amount of telegrams using this \gls{apci} normalised to the overall amount of telegrams observed within the period of this window.
According to \textcite{DIN_EN_50090-4-1} $44$ possible \glspl{apci} are defined, therefore introducing $44$ dimensions into the feature vector.
Compared to the other encoded features this would be a significant portion of the feature vector's dimensions. Consequently the problem of over fitting needs to be considered, similar as in the case of the payload length.
However, grouping the \gls{apci} expression is not as easy as introducing buckets for the payload length, since the \gls{apci} is a 
not a discrete measure, but rather categorical. Further, the \gls{apci} is more characteristic for a telegram as it determines directly the effect which the telegram has.
This fact combined with the categorical nature of this feature lead to the decision to not compress it further and include it fully.

To form the final feature vector all vectors produces by encoding the single features are concatenated and returned as one single vector.

\section{The Local Outlier Factor Analyser Module}
\label{sec:impl:lof}

\begin{comment}
\begin{itemize}
	\item purpose is to detect uncommon bus activity
	\item world view as well as agent based
	\item uses the \gls{lof} implementation of Scikit-learn \parencite{Pedregosa2011}
	%\item during training phase, grouped agent windows are extracted from \gls{influxdb} (effectively mimicking the collectors grouping algorithm)
	
	% feature vector
	\item for each window of each agent a \gls{fvect} is constructed from
		\subitem normalized seconds since the beginning of the year
		\subitem source addresses
		\subitem destination addresses
		\subitem priority distribution
		\subitem hop count distribution
		\subitem payload length distribution
		\subitem \gls{apci} usage distribution
	\item Both source and destination addresses are encoded as a vector of length 16
		\subitem each dimension in this vector represents the probability of occurrence of a address bit in the current window
		\subitem adaption of the hashing trick (cf. Section~\ref{sec:background:network:features:hashing})
		\subitem to reduce \gls{vect}, instead of using $2 \cdot 2^16 = 2 \cdot 65536 = 131072$ dimensions to model the \gls{pmf} of both address fields
		\subitem as a result only $2 \cdot 16 = 32$ dimensions to encode the addresses, without loosing information
	\item priority is encoded using an adoption of the OneHot encoding (cf. Section~\ref{sec:background:network:features:onehot}) to model the \gls{pmf} of the occurrence of priorities in the current window
		\subitem $4$ dimensions to encode the priorities \gls{pmf} (cf. Table~\ref{tab:background:bas:knx:proto:prio})
	\item hop count similarly encoded as priority, using an OneHot adaption
		\subitem $7$ dimension to encode the hop count \gls{pmf} (cf. Table~\ref{tab:background:bas:knx:proto:knx-standard}~and~\ref{tab:background:bas:knx:proto:ctrle})
	\item payload length uses a different adaption of the OneHot encoding (cf. Section~\ref{sec:background:network:features:onehot})
		\subitem the payload length (max $255$ Bytes) is divided into $10$ buckets to reduce the dimensions of the \gls{fvect}
		\subitem most analysed traffic has rather short payload
		\subitem therefore most telegrams will fit in the first or second bucket
		\subitem still clear distinction if packets with longer payload occur, just granularity gets lost
	\item \gls{apci} is model similar to priority and hop count using the same adaption of the OneHot encoding
		\subitem every of the possible \alert{37} \todo{(check if this is still valid, after the modifications of the baos lib)} \gls{apci} values is modelled as unique dimension
		\subitem overall representing the \gls{pmf} of occurrences per \gls{apci} values in the current window
		
	\item using this feature vector 2 models are trained per agent window
		\subitem one agent specific model
		\subitem one world model, which is trained using data from all agents (aka. the whole network)
	\item purpose of the agent model is to be able to detect traffic leakage
		\subitem i.e. traffic that is normal for one line (e.g. light switches and motion sensors) suddenly occurs in the line responsible for \gls{hvac} control
		\subitem agent model is highly trained and most probably highly sensitive, but can detect unusual behaviour which is normal for the network but not for the line
		\subitem world model is more general and not as sensitive, but cannot distinguish between purposes of lines
	\item analyser module stores 1 for being an outlier or 0 for not being an outlier in the \gls{influxdb} per window
		\subitem threshold is decided in analyser
		\subitem easier monitoring, but data get mangled doing this
		\subitem better might be to store the relative distance (output of \gls{lof}) as well
\end{itemize}
\end{comment}

A more sophisticated Analyser modules uses the \gls{lof} algorithm, an proximity-based approach to outlier or anomaly detection.
The main idea is outlined in Section~\ref{sec:concept:anal:lof}. The general concept is to detect uncommon or abnormal behaviour by comparing the current observation to prior observations and determine if it is significantly further away within the feature space.
To determine theses distances the \glsfirst{lof} implementation of \gls{sklearn} library is used with the feature vector as input, which is constructed as described in Section~\ref{sec:impl:vectoriser}.
The main advantage of the \gls{lof} algorithm consists in its ability to cope with unclean data, this means that the training data set can contain a small portion of malicious traffic without affecting the performance.

During the training phase the module may create a new base-line model or further train an existing one, whereby the construct of the base-line model consists for multiple sub-models. First there is a general world-view model used to compared to traffic from all Agents to. Second there are multiple Agent specific models, each for every Agent. All of these sub-models is managed using functions provided by the Base Module described in Section~\ref{sec:impl:base}.
The differentiation between the general world-view model and the individual Agent specific models is important to maintain a general overview of activities which are normal in the general context of the \gls{bas} network in question and simultaneously being able to identify minor changes of the activities in individual network segments.
Basically by having a general model and multiple highly specific ones a balance between low and high sensitivity is archived. 
The world-view model is less likely to react to small derivations and therefore reduce false positives. The Agent specific models, however, enable the system to detect anomalies caused by activities which are normal for one line, but extremely rare on another line.

To train these different models first data from \gls{influxdb} is queries for a chosen training period using the functions provided by the Base Module, then this data is grouped in the same way as the Collector would do and passed into the Vectoriser.
Finally, the resulting feature vectors are used to train the respective (sub-)models with \gls{sklearn}.

During the operational phase the module analyses incoming windows, by first passing them through the Vectoriser and then comparing them to two models: the world-view model and the respective Agent specific model for the Agent that captured that window.
For this the vector of the to-be-analysed window is compared to its \emph{MinPts} $100$ nearest neighbours using the \emph{Minkowski} distance with $p=2$, which equals to the Euclidean distance, according to the \gls{lof} algorithm as introduced by \textcite{Breunig2000}.
The resulting factor is then compared to a threshold, which is derived from the assume contamination of the training data and the training data itself. For the purpose of demonstration the contamination is assumed to be $10\%$.
Finally, the \gls{lof} and the result after comparing it to the threshold are pushed to the \gls{influxdb} database for both models.
The actual binary decision is important for alerting and can not be easily archived within the monitoring and alerting solution, as the threshold is not fixed.
However, the factor of \enquote{oulier-ness} is provided as well to allow for better over time monitoring of the abnormal activities within the network and to provide more insides.

\section{The Support Vector Machine Analyser Module}
\label{sec:impl:svm}

Another approach from the class of proximity-based approaches is the \gls{svm} Analyser module. It uses a special subclass of \glspl{svm}
to separate observations belonging to the \emph{normality} from outliers.
Whereby the concept of \gls{lof} is to determine outliers by comparing distances in the local cluster, \glspl{svm} define a border or decision surface of \emph{normality}. Every data point outside this border is considered an outlier.

This decision surface is defined during the training period, where \gls{rbf} kernel is fitted to cover to training data as best as possible. For this multiple iterations of fitting and measuring the effect are performed, whereby the upper bound of training errors and the lower bound of support vectors $nu$ is set to $0.01$. The influence $\gamma$ each sample has on the shape of the \gls{rbf} is determined automatically as $\gamma = \frac{1}{number of features}$, which follows the recommendation for one-class \gls{svm} by \gls{sklearn}.
The training data is acquired using the functions provided by the Base Module (see Section~\ref{sec:impl:base}) and then translated into the vector space using the Vectoriser.
The algorithms provided by \gls{sklearn} are used to train to sets of models involving one world-view model and several Agent specific models.

Once the training period is concluded the \gls{svm} models can be used in the operation phase to analyse incoming windows.
These windows are first converted using the Vectoriser. With the resulting vector the distances to the closest decision surface are then calculated, using both the world-view model and the respective Agent specific model.
A positive distance indicates a inlier, whereby a negative distance indicates an outlier.
Both distances are considered separately. This division allows to get general less sensitive measurement of abnormality within the network, while simultaneously also having the result of a model specifically trained for one line segment. Later one is able to identify traffic, which is foreign to the observed segment. This may provide insights to traffic leaking between segments or generally behaviour which might seem normal, but is alien to the specific segment.
Finally, both distances and the outlier or inlier decision for each distance are pushed to the \gls{influxdb}.

\section{The Entropy Analyser Module}
\label{sec:impl:entropy}

Opposed to the proximity-based approaches of the \gls{lof} and \gls{svm}, the Entropy Analyser evaluates the statistical distribution using the Shannon entropy. \parencite[cf.][]{Shannon1948}
For this it data is accumulated during the training period passed through the Vectoriser using the functions provided by the Base Module.
Unlike the proximity-based approaches the time sensitivity cannot be archived by using one dimension to separate clusters by time. Therefore this dimension is stripped from the feature vector. Instead multiple time buckets are defined over the seasonal period, with a base-line model for each of them. The time dimension is then used to determine which time bucket to use.
To smooth artefacts which might occur on the transition points from one bucket to another, the effective amount of buckets is doubled. Whereby the second half is shifted in time by half the bucket's length.
Consequently every relative time point in the season falls in exactly two overlapping buckets.
In the prototype implementation a seasonal period of one week is assumed and the bucket length is set to one hour. This results in $2 \cdot (7 \cdot 24) = 2 \cdot 168 = 336$ buckets. Each bucket contains the mean distribution per feature dimension.

Additional to the separation by time buckets, the Entropy Analyser also separates between one world-view model and multiple Agent specific models. This separation allows for finer sensitivity adjustments and therefore might provide better insights into abnormal activities based on the behaviour specific for one network segment.

During the operational phase the mean distribution stored in the base-line models is used to calculate the entropy when combined with the new incoming window for the world-view model and the respective Agent specific one.
For the calculation of the entropy the statistical functions of the \gls{scipy} library are used.
The resulting entropies is then pushed to the \gls{influxdb} as raw values. A threshold for the outlier/inlier decision is not applied.
