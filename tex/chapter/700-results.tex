% !TeX spellcheck = en_GB
Using the concepts described in Chapter~\ref{sec:concept}, a prototype was implemented to validate, test and proof it which is described in Chapter~\ref{sec:impl}.
In this Chapter this implementation shall be used to conduct a performance experiment on the implementation itself and the used algorithms.
For which first the experiment setup is described.

\section{Conducting the Performance Experiment}
\label{sec:results:experiment}

The in this thesis proposed concept for an online anomaly-based \gls{ids} tailored to \glspl{bas} was implemented as prototype, as described in Chapter~\ref{sec:impl}.
Using this prototype a full analysis pipeline was setup, which included the Collector, all four Analyser modules, and the Simulated Agent, as well as \gls{rabbitmq} as message broker and \gls{influxdb} as persistent data storage.
The Simulated Agent was used to inject prior recorded traffic into the system faster than real time.
The data was originally recorded between 2017-01-21 and 2017-02-21 on the third floor of the computer science building of the University Rostock.
Before injecting and analysing the data, it was then split into three parts. The first two weeks are dedicated to train the base-line models for the Analyser modules. The following week was used as validation, to ensure the algorithms are properly fitted to the data. Finally, the last week was modified to contain four scenarios which alter the behaviour of the line, as described in Section~\ref{sec:methods:gen-test}.
\newpage
These modifications are meant to resemble plausible attacks on \gls{bas} networks and therefore include:

\begin{enumerate}
	\item Injecting unusual network traffic by copying traffic from another day and time
	\item Performing a \gls{dos} attack
	\item Scanning all addresses of the network
	\item Introducing two new devices
\end{enumerate}

First, only the training data was imported via the Simulated Agent and pushed to the \gls{influxdb} by the Collector. This data was then used to train the different algorithms.
Then the remaining two parts of dataset were imported via the Simulated Agent and processed into statistical windows, these were then processed by the Collector, and relayed to the analytical modules. Finally, the Analyser compare the incoming windows to the base-line model and store the result in the \gls{influxdb}. From there the monitoring and alerting software \gls{grafana} can query the results and plot them into dynamic graphs, as well as send alerts based on predefined thresholds or rules.

During the import of the test dataset an increasing delay of queries to the \gls{influxdb} were observed.
\todo{...}

\todo{clustering helpful for slow estimation results in lof}

In this experiment the feature to simulate multiple Agents, using filter rules, was not used due to the fact that the dataset was merely recorded on one single line. Therefore, a further separation seemed unfeasible, as it would complicate the evaluation of the detection results without providing more insights.

\section{Experiment Results}
\label{sec:results:results}

Finally, in this section the detection results of the proposed algorithms are examined using the graphs generated with \gls{grafana}. They will be used to determine the quality of the detection results based on five criteria:

\begin{enumerate}
	\item General ability to the detect the attack
	\item Differentiation from background noise of the detection results
	\begin{enumerate}
		\item Average difference in detected outliers compared to the validation dataset
		\item Average difference in underlying decision metric compared to the validation dataset
	\end{enumerate}
	\item Response time 
	\item Persistence of detection
\end{enumerate}

These criteria will be evaluated by interpreting six different plots exported from the monitoring and alerting solution \gls{grafana}.
The first plot (a) shows the amount of telegrams per window over time as green staircase. Additionally the amount of telegrams using an unknown address (either as source or destination) are plotted in yellow.
The second graph (b) builds onto this and indicates how many unique addresses per window were found over time, separated by source addresses (green) and destination addresses (yellow).
Third (c) the number of windows considered outliers over time is shown, detected by the \gls{svm} (red) and using the \gls{lof} (blue). Whereby not single telegrams are evaluated but entire windows.
The fourth plot (d) displays the calculated entropy per window over time.
Finally, the fifth (e) and sixth (f) graph show the spread (difference between minimum and maximum) and average of the underlying metric for the \gls{lof} and the \gls{svm}.
In case of the \gls{lof} lower values indicate outlying observations. In case of the \gls{svm} the graph depicts the distance from the decision surface. Whereby a negative distance indicates an observation outside the borders of normality, an outlier, and a positive distance indicates and inlier. The higher the distance is, the more distinct is the classification.
As as general rule, a smaller spread suggests a clearer decision or tendency towards inlier or outlier over multiple windows.

The distinction by Agents can be ignored, as only one Agent was simulated in this experiment. 

\subsection{Detection of Unusual Network Traffic}
\label{sec:results:results:unusual}

The first modification applied to the test dataset was the injection of unusual traffic. 
For this a time span of five hours was replaced with traffic from the same line, but from different weekday and time of the day.
This effectively aims to test the seasonal- or time-sensitivity of the models.

Figure~\ref{fig:results:unusual} shows several metrics from the four different anomaly detection methods.
In \ref{fig:results:unusual:tc} a sudden increase in the observed amounts of telegrams per window is to observe, whereby telegrams with using unknown addresses were observed. This is to be expected since the five hours which replaced the original traffic were taken from the same line but from a more busy point of the week, hence the increase of transmitted telegrams.
The absence of unknown addresses, either as source or as destination, is also clearly indicated by Figure~\ref{fig:results:unusual:addr}.

Surprisingly, there is also no significant increase in the number of detected outliers neither by the \gls{svm}, nor using the \gls{lof}, as shown in Figure~\ref{fig:results:unusual:outlier}.
Even though the \gls{lof} shows an average of $0.77$ outliers, it can hardly be considered an significant increase when compared to the surrounding unmodified data and to the validation dataset in Appendix~\ref{app:metrics:validation}.
However, the spread of the \gls{lof} depicted in Figure~\ref{fig:results:unusual:lof} shows a small change downwards, which indicates less normal behaviour -- it is arguably not significant enough.

In contrast, Figure~\ref{fig:results:unusual:svm} even shows that the distance to the decision function of the \gls{svm} even increases positively, which means the analysed points are considered to represent the normality stronger. This is also reflected in the detection of no outliers with the exception of two small peaks.

The Entropy Analyser mostly calculates a entropy of infinity, which is here encoded as $100000$. This results is not limited to the period of the modification and therefore renders its results relatively meaningless.

\begin{figure}[H]
	\newcommand{\figwith}{0.49\textwidth}
	\newcommand{\figprefix}{unusual}
	\centering
	
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-tc.png}
		\caption{Telegrams (green) and telegrams with unknown addresses (yellow) over time}
		\label{fig:results:\figprefix:tc}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-unknown-addr.png}
		\caption{Amount of observed unknown source (green) and destination (yellow) addresses}
		\label{fig:results:\figprefix:addr}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-num-outliers.png}
		\caption{Number of detected outliers via SVM (red) and LOF (blue).}
		\label{fig:results:\figprefix:outlier}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-entropy.png}
		\caption{Calculated Entropy over time, infinity is encoded as $100 000$}
		\label{fig:results:\figprefix:entropy}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-lof-spread.png}
		\caption{Spread and average of the Local Outlier Factor, smaller numbers indicate outlier}
		\label{fig:results:\figprefix:lof}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-svm-spread.png}
		\caption{Spread and average of the distance to decision surface, negative distances are outlier}
		\label{fig:results:\figprefix:svm}
	\end{subfigure}
	
	\caption[Detection Results of Unusual Network Traffic]{Detection Results of Unusual Network Traffic from 2017-02-12 02:00 to 09:00 with a minimal time resolution of 2 minutes. The modified time range is marked as a grey box.}
	\label{fig:results:\figprefix}
	
\end{figure}

\begin{table}[H]
	\aboverulesep=0ex
	\belowrulesep=0ex
	\renewcommand{\arraystretch}{1.2}
	\newcolumntype{Y}{>{\centering\arraybackslash}X}
	
	\centering
	\begin{tabularx}{0.95\textwidth}{|l|Y|Y|Y|Y|}
		\toprule
		& \gls{lof} & \gls{svm} & Address & Entropy \\\midrule
		1. Attack detected & no & no & no & no \\\midrule
		2.a) Avg. difference in outliers  & $0.319$ & $0.013$ & 0 & n/a \\\midrule
		2.b) Avg. difference in metric & $2.65$ & $5.42$ & n/a & n/a \\\midrule
		3. Response time & 0s & 0s & n/a & n/a \\\midrule
		4. Persistence & n/a & n/a & n/a & n/a \\\bottomrule
	\end{tabularx}
	\caption[Detection results of unusual traffic]{Detection results of unusual traffic. Averages are compared to the validation dataset.}
	\label{tab:results:unusual}
\end{table}

\subsection{Detection of a DoS Attack}
\label{sec:results:results:dos}

The second modification applied to the test dataset, included a \gls{dos} attack, which flooded the entire line \code{3.4} with \code{A\_Restart} telegrams. As to expect the amount of telegrams with unknown addresses and count of unknown addresses jump right up, as shown in Figure~\ref{fig:results:dos:tc} and~\ref{fig:results:dos:addr}. This clearly indicates an anomaly, since the threshold for unknown addresses within the network should be set to zero.

Figure~\ref{fig:results:dos:outlier} illustrates, that the \gls{lof} and \gls{svm} outlier detection reacted equally prompt, by both classifying the entire period of the attack as outlier.
This is also reflected by the underlying metrics. The maximum of the \gls{lof} in Figure~\ref{fig:results:dos:lof} stays low during the entire attack. Similarly, the distance to the decision surface of the \gls{svm} resides in the high negatives implying a huge divergence from the normality.
Especially noteworthy is that the spread between minimum and maximum of the Local Outlier Factor as well as the distance to the decision surface falls close to zero during the attack, which indicates a high certainty of the classification.
The premature identification of the anomalies visible in Figure~\ref{fig:results:dos:outliers} could be caused by misalignment and small inaccuracies in the time windowing processes performed by the Collector.

The entropy, plotted in Figure~\ref{fig:results:dos:entropy}, does not change at all, neither during the modification nor outside of it.

\begin{figure}[H]
	\newcommand{\figwith}{0.49\textwidth}
	\newcommand{\figprefix}{dos}
	\centering
	
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-tc.png}
		\caption{Telegrams (green) and telegrams with unknown addresses (yellow) over time}
		\label{fig:results:\figprefix:tc}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-unknown-addr.png}
		\caption{Amount of observed unknown source (green) and destination (yellow) addresses}
		\label{fig:results:\figprefix:addr}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-num-outliers.png}
		\caption{Number of detected outliers via SVM (red) and LOF (blue).}
		\label{fig:results:\figprefix:outlier}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-entropy.png}
		\caption{Calculated Entropy over time, infinity is encoded as $100 000$}
		\label{fig:results:\figprefix:entropy}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-lof-spread.png}
		\caption{Spread and average of the Local Outlier Factor, smaller numbers indicate outlier}
		\label{fig:results:\figprefix:lof}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-svm-spread.png}
		\caption{Spread and average of the distance to decision surface, negative distances are outlier}
		\label{fig:results:\figprefix:svm}
	\end{subfigure}
	
	\caption[Detection Results of a DoS]{Detection Results DoS attack from 2017-02-13 08:40 to 09:50 with a minimal time resolution of 2 minutes. The time range of the attack is marked as a grey box.}
	\label{fig:results:\figprefix}
	
\end{figure}

\begin{table}[H]
	\aboverulesep=0ex
	\belowrulesep=0ex
	\renewcommand{\arraystretch}{1.2}
	\newcolumntype{Y}{>{\centering\arraybackslash}X}
	
	\centering
	\begin{tabularx}{0.95\textwidth}{|l|Y|Y|Y|Y|}
		\toprule
		& \gls{lof} & \gls{svm} & Address & Entropy \\\midrule
		1. Attack detected & yes & yes & yes & no \\\midrule
		2.a) Avg. difference in outliers  & $0.549$ & $0.997$ & $254$ & n/a \\\midrule
		2.b) Avg. difference in metric & $-1.730$ & $-39.46$ & n/a & n/a \\\midrule
		3. Response time & 0s & 0s & 0s & n/a \\\midrule
		4. Persistence & $100$\% & $100$\% & $100$\% & n/a \\\bottomrule
	\end{tabularx}
	\caption[Detection results of the DoS attack]{Detection results of the \gls{dos} attack.}
	\label{tab:results:dos}
\end{table}

\subsection{Detection of a Network Scan}
\label{sec:results:results:scan}

The third modification to be applied to the test dataset was a network scan where the entire \gls{knx} address space was scanned using a management routine every \gls{knx} device must implement. Every present device will respond with an appropriator answer.

Similar to the \gls{dos} attack produces a scan of the entire network significant more traffic than normal operation, which is indicated by Figure~\ref{fig:results:scan:tc}. Further a lot of traffic targeting prior unknown addresses is inevitable, causing the Address Analyser to detect such behaviour. (see Figure~\ref{fig:results:scan:addr})

In contrast to the clear identification by the Address Analyser stands the \gls{lof} Analyser, which identifies the attack but as outlier in Figure~\ref{fig:results:scan:outlier}, but the separation from the background noise is not significant. Similar is to observe in the underlying \gls{lof} metric depicted in Figure~\ref{fig:results:scan:lof}. However, it is curious that even though the average of the \gls{lof} is stable compared with the surrounding background noise, the spread decreases drastically.

On the other hand, the \gls{svm} identifies the threat flawless. The low background noise and the clear detection as outlier in Figure~\ref{fig:results:scan:outlier} allows for a certain identification. This is supported by a sharp drop into the negative of the distance to the decision surface as shown in Figure~\ref{fig:results:scan:svm}, distinctly indicating an outlier.

The calculated entropy compared to the training data is continuously infinite, making it unfit to draw any conclusions from.


\begin{figure}[H]
	\newcommand{\figwith}{0.49\textwidth}
	\newcommand{\figprefix}{scan}
	\centering
	
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-tc.png}
		\caption{Telegrams (green) and telegrams with unknown addresses (yellow) over time}
		\label{fig:results:\figprefix:tc}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-unknown-addr.png}
		\caption{Amount of observed unknown source (green) and destination (yellow) addresses}
		\label{fig:results:\figprefix:addr}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-num-outliers.png}
		\caption{Number of detected outliers via SVM (red) and LOF (blue).}
		\label{fig:results:\figprefix:outlier}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-entropy.png}
		\caption{Calculated Entropy over time, infinity is encoded as $100 000$}
		\label{fig:results:\figprefix:entropy}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-lof-spread.png}
		\caption{Spread and average of the Local Outlier Factor, smaller numbers indicate outlier}
		\label{fig:results:\figprefix:lof}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-svm-spread.png}
		\caption{Spread and average of the distance to decision surface, negative distances are outlier}
		\label{fig:results:\figprefix:svm}
	\end{subfigure}
	
	\caption[Detection Results of a Network Scan]{Detection Results of Network Scan from 2017-02-13 20:45 to 21:17 with a minimal time resolution of 30 seconds. The time range of the attack is marked as a grey box.}
	\label{fig:results:\figprefix}
	
\end{figure}

\begin{table}[H]
	\aboverulesep=0ex
	\belowrulesep=0ex
	\renewcommand{\arraystretch}{1.2}
	\newcolumntype{Y}{>{\centering\arraybackslash}X}
	
	\centering
	\begin{tabularx}{0.95\textwidth}{|l|Y|Y|Y|Y|}
		\toprule
		& \gls{lof} & \gls{svm} & Address & Entropy \\\midrule
		1. Attack detected & no & yes & yes & no \\\midrule
		2.a) Avg. difference in outliers  & $0.029$ & $0.549$ & $5 \cdot 10^{3}$ & n/a \\\midrule
		2.b) Avg. difference in metric & $-2.10$ & $-32.97$ & n/a & n/a \\\midrule
		3. Response time & n/a & 0s & 0s & n/a \\\midrule
		4. Persistence & n/a & $100$\% & $100$\% & n/a \\\bottomrule
	\end{tabularx}
	\caption[Detection results of the network scan]{Detection results of the network scan.}
	\label{tab:results:scan}
\end{table}

\subsection{Detection of New Devices}
\label{sec:results:results:newdevice}

The fourth and final modification applied to the test dataset contained two new devices. The required data was taken from the same line, but from a different day, only the date and the source address were altered. This traffic was injected over the whole target day, keeping the frequency of the original traffic.

As traffic resembling an actual \gls{knx} device was injected using prior not used addresses, the graph in Figure~\ref{fig:results:newdevice:tc} shows only small amounts traffic with unknown addresses, when compared to \gls{dos} attacks or network scans.
However, Figure~\ref{fig:results:newdevice:addr} illustrates clearly activity with of unknown devices.
Since the alert threshold for both metrics is zero, an alert would be raised.

Opposed to that are the results of the outlier detection using \gls{lof} and the \gls{svm}, as shown in Figure~\ref{fig:results:newdevice:outlier}.
With only a difference of $0.055$ compared to the validation dataset no significant increase in outliers was detected by the \gls{lof} Analyser.
Similarly, the \gls{svm} identified on average $0.005$ outliers over the period of modification, which resembles a drop of $0.002$ when compared to the validation dataset.
Additionally, the spread for both the \gls{lof} and the distance to the decision surface in Figure~\ref{fig:results:newdevice:lof} and~\ref{fig:results:newdevice:svm} is considerably moderate, not indicating a strong tendency to either outlier or inlier.
Therefore, it is to conclude that neither the \gls{lof} nor the \gls{svm} detected this modification.
Although the occurrence of unknown addresses and outliers detected by the \gls{svm} show some timely correlation.

As in the previous modifications, no conclusion can be drawn from the Entropy Analyser as it is continuously detecting an infinite value.

The persistence and response time can not be taken into account for this modification. Even though the modification was applied over the course of the whole day, the device were not sending traffic right from the beginning of the day, therefore not being detectable on any layer above the physical.

\begin{figure}[H]
	\newcommand{\figwith}{0.49\textwidth}
	\newcommand{\figprefix}{newdevice}
	\centering
	
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-tc.png}
		\caption{Telegrams (green) and telegrams with unknown addresses (yellow) over time}
		\label{fig:results:\figprefix:tc}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-unknown-addr.png}
		\caption{Amount of observed unknown source (green) and destination (yellow) addresses}
		\label{fig:results:\figprefix:addr}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-num-outliers.png}
		\caption{Number of detected outliers via SVM (red) and LOF (blue).}
		\label{fig:results:\figprefix:outlier}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-entropy.png}
		\caption{Calculated Entropy over time, infinity is encoded as $100 000$}
		\label{fig:results:\figprefix:entropy}
	\end{subfigure}
	\\[1.5mm]
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-lof-spread.png}
		\caption{Spread and average of the Local Outlier Factor, smaller numbers indicate outlier}
		\label{fig:results:\figprefix:lof}
	\end{subfigure}
	\hfil
	\begin{subfigure}[b]{\figwith}
		\includegraphics[width=\textwidth]{figures/700-results/\figprefix-svm-spread.png}
		\caption{Spread and average of the distance to decision surface, negative distances are outlier}
		\label{fig:results:\figprefix:svm}
	\end{subfigure}
	
	\caption[Detection Results of New Devices]{Detection Results of New Devices from 2017-02-13 23:00 to 2017-02-15 01:00 with a minimal time resolution of 10 minutes. The time range of the modification is marked as a grey box.}
	\label{fig:results:\figprefix}
	
\end{figure}

\begin{table}[H]
	\aboverulesep=0ex
	\belowrulesep=0ex
	\renewcommand{\arraystretch}{1.2}
	\newcolumntype{Y}{>{\centering\arraybackslash}X}
	
	\centering
	\begin{tabularx}{0.95\textwidth}{|l|Y|Y|Y|Y|}
		\toprule
		& \gls{lof} & \gls{svm} & Address & Entropy \\\midrule
		1. Attack detected & no & no & yes & no \\\midrule
		2.a) Avg. difference in outliers  & $0.055$ & $-0.002$ & $0.03$ & n/a \\\midrule
		2.b) Avg. difference in metric & $0.32$ & $-0.13$ & n/a & n/a \\\midrule
		3. Response time & n/a & ($4.5$h) & ($6.83$h) & n/a \\\midrule
		4. Persistence & n/a & ($0.54$\%) & ($3.23$\%) & n/a \\\bottomrule
	\end{tabularx}
	\caption[Detection results of new devices]{Detection results of two new devices in the network.}
	\label{tab:results:newdevice}
\end{table}

\section{Summary of the Detection Results}
\label{sec:results:summary}

\begin{comment}
\begin{itemize}
	\item 3 out of 4 attacks were successfully detected
	\item 2 of those by machine learning algorithms
	\item to note, that lof and svm were trained with normalised data
		\subitem traffic rates not into account
		\subitem only change in composition/structure can be recognised
		\subitem changes in transmission rate should be monitored in Grafana
		
	\item \gls{svm} is the best performing algo
		\subitem low background noise
		\subitem detected both direct attacks
		\subitem sharp decision surface
		\subitem small model, quick detection/estimation
		\subitem model training is computational intensive
				
	\item \gls{lof} does not perform well
		\subitem requires at least further tuning
		\subitem high background noise (cf. Figure~\ref{fig:results:validation:outlier})
		\subitem detected only \gls{dos} reliably
		\subitem almost detected scan, but was not clear distinguishable from background noise
		\subitem large model, longer computation time for detection
		\subitem nearest neighbour search is perform every time -> computational heavy, especially for large datasets
		
	\item addr analyser
		\subitem simple method to any drastic change involving unknown addresses in the network
		\subitem good supplement
		\subitem low resource overhead: small model, quick detection, fast training
	
	\item entropy was useless
		\subitem mostly entropy of inf calculated
\end{itemize}
\end{comment}

Altogether, three out of four attack scenarios could be detected using a variety of algorithms, as summarised in Table~\ref{tab:results:conclusion}. Two of those would have been detected using the unsupervised machine learning algorithms \gls{lof} and \gls{svm}.
It is to note that both rely on the feature vector and operated on normalised data, which correspondingly does not account for absolute rates.
Under those circumstances only the change in composition or structure of the traffic can be recognised by these algorithms. As effect they can handle changing window lengths without requiring re-training.
Also, as stated earlier absolute changes in e.g. transmission rate can be monitored easily in exiting monitoring and alerting solutions.

Another module relying on the feature vector and normalised data is the Entropy Analyser.
In contrast to the \gls{lof} and \gls{svm} it was not able any of the four modifications. Moreover, it calculated an infinite entropy for almost the entire dataset, which would represent an infinite information gain. (cf. Figure~\ref{fig:results:validation:entropy})
This behaviour might be caused by a high sensitivity, but renders the results of this Analyser modules useless.

On the other end of the spectrum resides the \gls{svm} Analyser. It detected both direct attacks reliably and outputs relatively low background noise and false-positives, therefore providing a sharp decision surface.
The trained model consumes with $504 KiB$ comparably less disk space and shows fast classification results. Although, the training process proved to be computational more intensive.

Proceeding, the \gls{lof} Analyser required further adjustments as it only detected on attack reliably.
It shows high amount of background noise and false-positives. (cf. Figure~\ref{fig:results:validation:outlier}) The detection \gls{dos} was clearly identifiable and also for the network scan a change in pattern of the \gls{lof} output could be observed, but was not explicitly distinguishable from the background noise.
Further, during the experiment it was observed that the classification with \gls{lof} takes the longest among all algorithms, which was counteracted by scaling this Analyser Module.
Moreover, the trained model requires the most disk space with $151 MiB$, even though the training process is not computational intensive.

Lastly, the Address Analyser does not rely on the feature vector, it rather compares the observed addresses to a pre-trained list of known addresses. This is a good supplement and an easy way to discover new unknown devices and also to recognise attacks like network scans, which relies on probing all address within a specific range.
By relying on this simple working principle it only requires a small model with $1.4 KiB$ and is not computational intensive, neither during training nor operation.

\begin{table}[H]
	\aboverulesep=0ex
	\belowrulesep=0ex
	\renewcommand{\arraystretch}{1.2}
	\newcolumntype{Y}{>{\centering\arraybackslash}X}
	
	\centering
	\begin{tabularx}{0.95\textwidth}{|l|Y|Y|Y|Y|}
		\toprule
		& \gls{lof} & \gls{svm} & Address & Entropy \\\midrule
		Unusual traffic & no & no & no & no \\\midrule
		\gls{dos} attack & yes & yes & yes & no \\\midrule
		Network Scan & no & yes & yes & no \\\midrule
		New Devices & no & no & yes & no \\\midrule
	\end{tabularx}
	\caption[Summary of detection results]{Summary of detection results.}
	\label{tab:results:conclusion}
\end{table}

\section{Conclusion}
\label{sec:results:conclusion}

\begin{enumerate}
	\item \enquote{[...] anomaly detection methods seem especially applicable to SCADA system security which are characterized by routine and repetitious activities.} \parencite{Yang2006} Is it also a good fit for BAS? Does it make sense?
	\item How can anomaly detection identify different attack vectors in BAS:
		\subitem new devices
		\subitem (high) traffic load
		\subitem network problems
		\subitem configuration changes
		\subitem reconnaissance (network scan) 
	\item In which way does anomaly detection need to account for different seasons? And how high is the precision improvement compared to a model that is not season-sensible?
	\item Does the additional in-band traffic influences normal operations?
	\item Does the additional in-band traffic produces new attack surfaces?
	\item Which anomaly detection/evaluation method Ã®s the best for BAS?
	\item Which data reduction is feasible and in which part of the system does it makes sense? (in the data collection agent)
\end{enumerate}

\begin{itemize}
	\item In conclusion
	\item a modular and extensible software architecture was proposed, which allows for...
	\item The here proposed system and architecture allows for efficient and scalable online monitoring of \gls{bas}.
	\item Its modularity allows it to be used as foundation for further investigation and implementation of different algorithms, not just limited to anomaly detection.
	\item Along with the system and architecture a selection of machine learning algorithms were described, evaluated for the use in anomaly-based \glspl{ids}, and finally tested within a prototype of the proposed system.
	\item The prototype was implemented showcasing the capabilities of the algorithms as well as the architecture at the example of a real world \gls{knx} installation.
	\item Even though not every here tested model was able to detect all modifications or attacks, it was shown that anomaly detection methods also work in areas were traffic is hugely impacted by human behaviour and object to fluctuations and changes.
	\item The sensitivity to different kinds of attacks was illustrated in Section~\ref{sec:results:results}.
	\item In this overview it is to note, that the Entropy Analyser did not detect any of the scenarios. Further when looking into the corresponding graphs of this module in Section~\ref{sec:results:results} and Appendix~\ref{app:metrics}, it becomes evident that the calculated entropy is nearly always infinite. (Encoded as \(100 000\)) 
	\item This leads to the assumption that either the method is too sensitive to changes in the statistical distribution, or the implementation is erroneous. Given the result data from the experiment both possibilities need to be considered, since spikes of assumedly correct calculated entropy exist.
	
	\item Corresponding to the summary in Table~\ref{tab:results:conclusion}, it is to note, that modifications that do not alter the network traffic dramatically were less likely to be detected. This includes the \emph{unusual traffic} and \emph{new device} attack, which are rather shifting and copying records.
	\item Reason for this could be, that the network traffic in the dataset does not vary to much over time, but rather stays constant.
	\item Meaning, that the traffic used as source for the destination does not differ too much from the targeted time frame of the modification.
	\item Another explanation could be, that the ...
	
	\item Possible factor influencing the detection results include the parameters used for training the different models and the construction of the feature vector.
	\item Especially the \gls{lof} seems to be rather sensitive to the number of neighbours an observation is compared to. This parameter need to be adjusted in accordance to the amount of training data and the contamination of it. Further, with an increased number of neighbours and an larger training set the precision may rise, but the time complexity is impacted directly by \(O(n)\) in the worst case.
	\item For the \gls{svm} the predominantly important factor seems to be the type of function used to form the decision surface and how tight it is fit to the training data. Opposed to \gls{lof}, the performance is not influence by the size of the training data during operation, therefore a larger dataset can be used without considering performance issues.
	\item However, the most influential factor for all algorithms is the feature vector. Its construction and the projection of features to dimensions heavily impacts the way observations can be compared.
	\item In this work the focus was to emphasise critical \gls{knx} features. For this dimensionality was reduced using common methods. (see Section~\ref{sec:impl:vectoriser}) In hindsight these methods should have also been applied to the \gls{apci} feature of \gls{knx}, as it is the most dominant one with regards to occupied dimensions. Not only does this unnecessarily increase the required compute time, but can shift weights towards this single feature and therefore reduce the sensitivity to changes in other features.	
	
	\item Unfortunately, the seasonal-sensitivity was not testable due to limited test data.
		\subitem the part of the building the test data was acquired from was not subject to dramatic changes in usage during the examined test period.
		\subitem therefore a qualified answer for the requirement of strong seasonal sensitivity cannot be given at this point and would require extensive testing on a larger dataset.
		\subitem However, the introduction of a time-dependent dimension in the feature vector, will allow the base-line model to be fit tighter to what the \emph{normality} might be. Consequently, considering such a dimension in the models might be advised, in case such sensitivity might be required.
		
	\item Given the detection rate, the applied data reduction down to windows containing only statistical distribution seem to be sufficient for detection changes in the composition of the network traffic.
	\item By normalising all dimensions of the feature vector to their respective maximum within the currently observed window, the models could be trained independent from the length of the window. On the one hand, this allows the window length to be adjusted dynamically.
	On the other hand, the absolute values are lost and do not allow to detect changes of a general increase or decrease of traffic. 
	\item To a certain extend this might prevent the system from detecting failure of large portions of a line in a way were the composition of the traffic is not noticeably influence.
	\item Although this can introduce a blind spot within the observation capabilities, the change of the absolute of traffic should be monitored using existing monitoring and alerting solutions, as already mentioned.
	
	\item However, it is to consider if the limitation to in-band monitoring messages is feasible. As it is not uncommon in \gls{knx} installations to use \gls{ip} gateways to bridge line-segments, a full-take of the traffic would be available via high-speed backbones. Thus, allowing the Collector easily to tap into this stream and use it directly for further analysis.
		
	\item Since the tests in this thesis were not conducted within a real \gls{knx} installation, a comprehensive estimation on how regular traffic is influenced cannot be given
	% \item Then again, the general observed bandwidth utilisation in the available datasets was perceived to be relatively low.
	\item Complementary to the here introduced system, \textcite{Jung2018} developed and tested an Agent capable of observing live traffic and aggregating the gathered data to statistical windows or flows.
	\item He shows that the additional load produced by exporting flow records can be kept relatively low around 20\%, when window length and export timeout are chosen according to the network utilisation.
	\item For this it needs to be considered, if a constant relative low bus utilisation is desirable, or if the flow exports should be buffered and then burst-transmitted to the Collector. Later one adds the risk of small delays in the network during transmission of the flows.
	
	\item Another important topic to consider is the additional security risk introduced by exporting statistical windows or flows in-band to a Collector.
	\item As shown by \textcite{Mundt2012}, \gls{bas} like \gls{knx} networks can be exploited to harness sensitive user and behavioural data.
	\item At the time of this thesis, it is not common to deploy encryption in \gls{knx} networks. As a result attacker are not able to gain more insights from listening to flow or window exports, if they are eaves dropping a single network segment like a line, since this traffic is unprotected anyway. The only additional potentially introduced security vulnerability lies by the Collector and the backbone lines. In case attacker would gain access to these recordings or lines, they would gain insights to the general activity and the installed device of the building.
	\item Consequently, the computer system running the Collector and analytical modules, as well as the backbone lines require particular well considered security mechanisms.
	\item before used in production system, encryption should be implemented.
	\item Even though the risks for security and privacy are recognised in this work, it 
	
	\item Finally, ...
	
	
\end{itemize}
