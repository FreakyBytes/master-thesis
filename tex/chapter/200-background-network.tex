% !TeX spellcheck = en_GB

\todo{general introductory sentence about network mon}

\section{Intrusion Detection Systems (IDS)}
\label{sec:background:network:ids}

\begin{comment}
		\begin{itemize}
		\item \enquote{A good network intrusion detection system (IDS) can have an enormous positive impact on the overall security of your oragnization} \parencite{Northcutt2005}
		\item \enquote{By detecting malicious activity, network intrusion detection enables you to identify and react to threats against your environment, as well as threasts that your hosts might be directing at hosts on other networks.} \parencite[p. 201]{Northcutt2005}
		\item \enquote{Without intrusion detection, you may never know about an attack that doesn't damage your host, but simply extracts information[...]. Without intrusion detection, you will be unaware of these events until it's much too late.} \parencite[p. 202]{Northcutt2005}
		\item Not only identifies attacks, but also reveals attack attempts and probing \parencite[p. 202]{Northcutt2005}
		\item 
	\end{itemize}
\end{comment}

Good network security consists of many parts, covering a huge variety of methods and functions. \hint{This sounds weird and artificial.}
As one of these parts \enquote{a good \gls{ids} can have an enormous positive impact on the overall security [...]}. \parencite{Northcutt2005}
The basic idea is to monitor the network traffic and therefore be able to detect malicious activities.
This allows the \gls{ids} to detect and identify threasts against the own environment or theats ones own hosts impose to other environments, due to an infection with malware.
Without intrusion detection one might never detect attacks, that do not influence normal operations but for instance extract information.

Further, \gls{ids} may also identify attack attempts and probing, allowing to further harder the environment and take precautions.

Nowadays \glspl{ids} are a commonly deployed in \gls{ip} networks. There are different kinds of \glspl{ids}, which can be discriminated by working principle. The following section is giving an overview of the most common technologies. \parencite[cf.][pp.~201-202]{Northcutt2005}

\subsection{Signature Intrusion Detection Systems (SIDS)}
\label{sec:background:network:ids:sig}

\begin{comment}
	\begin{itemize}
		\item \enquote{[...] IDS signature is a pattern you are looking for in traffic.} \parencite{Northcutt2005}
		\item \enquote{When a signature for an attack matches observed traffic, an alert is generated, or the event is otherwise recorded.} \parencite{Northcutt2005}
		\item \enquote{Many signatures are protocol or application specific;} \parencite{Northcutt2005}
		\item requires to update a curated set of signatures regularly \parencite{Northcutt2005}
		\item \enquote{Signature development is always a balancing act. A specific signature might be extremely accurate in identifying a particular attack [...] [but] If an attacker slightly modifies the attack, the signature might not be able to identify it at all.} \parencite{Northcutt2005}
			\subitem precision/recall balance
		\item \enquote{Every intrusion detection system generates false positives.} \parencite[p.~205]{Northcutt2005}
		\item \enquote{By selecting a solid IDS product and tuning your sensors, you can reduce false positives, but you can't completely eliminate them.} \parencite[p.~205]{Northcutt2005}
		\item \enquote{[...], but each false negative is a legitimate attack or incident that IDS failed to notice.} \parencite[p.~206]{Northcutt2005}
		\item \enquote{Besides false positives, you might also have alerts that address legitimate problems that you simply don't care about.} (Unwanted Alerts) \parencite[p.~206]{Northcutt2005}
		\item 
	\end{itemize} 
\end{comment}

One of the more traditional \gls{ids} principles are signature based systems.
They work by comparing patterns (signatures) of malicious traffic to the observed traffic. When a pattern matches an alert is raised.
This requires many protocol and application specific signatures, which are difficult to create and maintain, since a good balance between being too generic and being too specific needs to be found.

A signature which is too generic, may cause many false positives. This might lead to an elimination of any positive effect of the \gls{ids}, since real alerts become impossible to spot among the false positives.
In contrast, however, a signature which is too specific might suffer from a too narrow scope. Meaning, that it might be really precise in classifying one particular attack, but fails to do so if an attacker just slightly modifies it.

As a result, a valuable \gls{ids} solution is characterised by an balanced precision/recall ratio. This can be archived by using a good signature database and fine tuning the sensors.
However, it is to note, that every \gls{ids} will produce false positives. This derives out of the consideration, that each false negative is an legitimate attack, which was not detected. Therefore a slight tendency to (over) sensitivity is sensible.

Other than false positives, there might be legitimate alerts that are of no interest. These unwanted alerts should be blocked, to not numb \alert{(is this the right word here?)} the system administrator. \parencite[cf.][pp.~205-206]{Northcutt2005}
	
\subsection{Anomaly-based Intrusion Detection Systems (ABIDS)}
\label{sec:background:network:ids:anomaly}
	
	\begin{itemize}
		\item \enquote{establishing baselines of normal network activity over a period of time, then detecting significant deviations from the baseline.} \parencite[p. 203]{Northcutt2005}
		\item \enquote{[...] later, if the IDS sensor sees a high volume of traffic involving a previously unused service on a host, this could indicate a distributed denial of service (DDoS) attack against the host or a compromised host providing a new service.} \parencite{Northcutt2005}
		\item \enquote{[...] drawback of this type of anomaly detection is that the baseline needs to be updated constantly to reflect authorized changes[...]} \parencite{Northcutt2005}
		\item \enquote{If the baseline can be kept current, or if the environment is quite static and the baseline changes rarely, anomaly detection can be extremely effective at identifying certain types of attacks [...]} \parencite{Northcutt2005}
		\item \textbf{\enquote{Unfortunately, this type of anomaly detection cannot identify most other types of attacks, so it is best used to complement other IDS technologies.}} \parencite{Northcutt2005}
		\item \enquote{[...] can [...] identifying certain previously unknown instances of attacks [...]} \parencite{Northcutt2005}
		\item \enquote{cannot determine the intent of an attack} \parencite{Northcutt2005}
		
	\end{itemize}

Another major technology used in \glspl{ids} is anomaly detection. (cf. Section~\ref{sec:background:network:novelty})
Unlike signature based methods (cf. Section~\ref{sec:background:network:ids:sig}) it does not require an predefined set of rules, rather deploying anomaly detection it is able to auto-generate rules specific for the monitored network and then perform whitelisting using these rules.

This is archived by having a so called training phase before deploying the \gls{ids}. During the training phase a baseline model is established using the normal network activity over period of time. It is to note, that this period should be long enough to cover normally recurring usage cycles, e.g. day-time/night-time, weekday/weekends, etc.

After an successful training phase the \gls{ids} uses the generated baseline model to compare bypassing against it. If it detects a significant deviation from it, an alert is raise as it indicates unusual network activities.
	
	\subsection{State-based Intrusion Detection Systems\hint{??}}
	\label{sec:background:network:ids:state}
	
	\begin{itemize}
		\item cf. \textcite{Whitman2009} page 306 
	\end{itemize}
	
\section{Netflow}
\label{sec:background:network:netflow}

	\begin{itemize}
		\item active
			\begin{itemize}
				\item inject additional traffic to measure things \parencite{Hofstede2014}
				\item e.g. ping, traceroute
			\end{itemize}
		\item passive
			\begin{itemize}
				\item "observe existing traffic as it passes by" \parencite{Hofstede2014}
				\item e.g. packet capture "This method generally provides most insight into the network traffic, as complete packets can be captured and further analyzed. However, in high-speed networks with line rates of up to 100 Gbps, packet capture requires expensive hardware and substantial infrastructure for storage and analysis." \parencite{Hofstede2014}
				\item "Another passive network monitoring approach that is more scalable for use in high-speed networks is flow export, in which packets are aggregated into flows and exported for storage and analysis." \parencite{Hofstede2014}
				\item  "a set of IP packets passing an observation point in the network during a certain time interval, such that all packets belonging to a particular flow have a set of common properties." \parencite{Claise2013}
				
			\end{itemize}
	\end{itemize}

\section{Anomaly, Outlier, and Novelty Detection}
\label{sec:background:network:novelty}

	\subsection{Classification of Anomaly, Outlier, and Novelty Detection Approaches}
	\label{sec:background:network:novelty:class}
	
	\begin{itemize}
		\item cf. \textcite{Hodge2004}
		\item Type 1: "Determine the outliers with no prior knowledge of the data" \parencite{Hodge2004}
			\subitem "unsupervised clustering" \parencite{Hodge2004}
			\subitem "assumes that errors or faults are separated from the ‘normal’ data and will thus appear as outliers" \parencite{Hodge2004}
			\subitem " predominantly retrospective and is analogous to a batch-processing system" \parencite{Hodge2004} - Requires a bunch of data to begin with
			\subitem " requires that all data be available before processing" \parencite{Hodge2004}
			\subitem "data is static" \parencite{Hodge2004}
			\subitem "two sub-techniques" \parencite{Hodge2004}
			\subitem -> "outlier diagnostic approach highlights the potential outlying points" \parencite{Hodge2004} and "prune the outliers and fit their system model to the remaining data until no more outliers are detected" \parencite{Hodge2004}
			\subitem -> "accommodation that incorporates the outliers into the distribution model generated and employs a robust classification method" \parencite{Hodge2004} which "can withstand outliers in the data and generally induce a boundary of normality around the majority of the data" \parencite{Hodge2004}
			\subitem "Non-robust methods are best suited when there are only a few outliers in the data set" \parencite{Hodge2004}
			\subitem "a robust method must be used if there are a large number of outliers to prevent this distortion" \parencite{Hodge2004}
		\item Type 2: "Model both normality and abnormality." \parencite{Hodge2004}
			\subitem "supervised classification" \parencite{Hodge2004}
			\subitem "requires pre-labelled data" \parencite{Hodge2004}
			\subitem "The entire area outside the normal class represents the outlier class" \parencite{Hodge2004}
			\subitem "best suited to static data" \parencite{Hodge2004} "classification needs to be rebuilt from first principles if the data distribution shifts" \parencite{Hodge2004}
			\subitem "can be used for on-line classification" \parencite{Hodge2004} learning while new samples are classified
			\subitem "require a good spread of both normal and abnormal data" \parencite{Hodge2004}
			\subitem "classification is limited to a ‘known’ distribution" \parencite{Hodge2004}
			\subitem "cannot always handle outliers from unexpected regions" \parencite{Hodge2004}
		\item Type 3: "Model only normality [...]" \parencite{Hodge2004}
			\subitem "novelty detection" \parencite{Hodge2004} " semi-supervised recognition or detection task" \parencite{Hodge2004}
			\subitem "needs pre-classified data but only learns data marked normal" \parencite{Hodge2004}
			\subitem "suitable for static or dynamic data" \parencite{Hodge2004}
			\subitem "can learn the model incrementally as new data arrives" \parencite{Hodge2004}
			\subitem "aims to define a boundary of normality" \parencite{Hodge2004}
			\subitem "requires the full gamut of normality to be available for training to permit generalisation" \parencite{Hodge2004} but "requires no abnormal data" \parencite{Hodge2004}
			\subitem this is good, because "Abnormal data is often difficult to obtain or expensive in many fault detection domains [...]" \parencite{Hodge2004}
			\subitem "as long as the new fraud lies outside the boundary of normality then the system will be correctly detect the fraud" \parencite{Hodge2004}
		
	\end{itemize}

	\subsection{Statistical techniques}
	\label{sec:background:network:novelty:stat}
	
	\begin{itemize}
		\item "Some [...] are applicable only for single dimensional data sets" \parencite{Hodge2004}
		\item "requires no user parameters as all parameters are derived directly from data" \parencite{Hodge2004}
		\item e.g. "Grubbs’ method (extreme studentized deviate) (Grubbs, 1969) which calculates a Z value as the difference between the mean value for the attribute and the query value divided by the standard deviation for the attribute where the mean and standard deviation are calculated from all attribute values including the query value. The Z value for the query is compared with a 1\% or 5\% significance level" \parencite{Hodge2004} also cf. \textcite{Grubbs1969}
	\end{itemize}
	
	\subsection{Proximity-based techniques}
	\label{sec:background:network:novelty:prox}
	
	\begin{itemize}
		\item "no prior assumptions about the data distribution" \parencite{Hodge2004}
		\item "not feasible for high dimensionality data sets" \parencite{Hodge2004}
		
	\end{itemize}

	\subsection{K-nearest neighbour}
	\label{sec:background:network:novelty:knn}
	
	\begin{itemize}
		\item \enquote{determines whether or not a point lies in a sparse region of the feature space by computing the sum of the distances to the k-nearest neighbours of the point} \parencite{Eskin2002}
		\item \enquote{[...] points in dense regions will have many points near them and will have a small k-NN score} \parencite{Eskin2002}
		\item \enquote{main problem [...] is that it is computationally expensive to compute the k-nearest neighbours of each point} \parencite{Eskin2002}
		
	\end{itemize}
	
	\subsection{Local Outlier Factor}
	\label{sec:background:network:novelty:lof}
	
	\begin{itemize}
		\item Proximity-based technique
		\item seems to be a good fit cf. \textcite{Lazarevic2003}
		\item works on unclean training data
		\item good selection of vectors and distance functions is required
		\item 11\% of the first 500 telegrams in \verb|eiblog.txt| are considered outliers
		\item density based
		\item base paper \textcite{Breunig2000}
			\begin{itemize}
				\item determines for each sample in the data-set its degree of \enquote{outlier-ness} \parencite{Breunig2000}
				\item definition outlier: \enquote{An outlier is an observation that deviates so much from other
					observations as to arouse suspicion that it was generated by a
					different mechanism.} \parencite{Hawkins1980}
				\item local approach
					\subitem therefore better than k-NN, because it accounts for different density of different cluster
					\subitem \enquote{most existing work in outlier detection lies in the field of statistics} \parencite{Breunig2000}
					\subitem counteracts different density of clusters \parencite{Breunig2000}
					\subitem outlier = point is further away from its nearest points than the other points are away from each other \parencite{Breunig2000}
			\end{itemize}
	\end{itemize}
	
	\subsection{Support Vector Machines}
	\label{sec:background:network:novelty:svm}
	
	\begin{itemize}
		\item \enquote{The task of classification is to find a rule, which, based on external observations, assigns an object to one of several classes} \parencite{Muller2001}
		\item separate vector space by decision surfaces, setting boundaries between categories \parencite{Muller2001}
		\item \enquote{a learning algorithm for problems which are separable by hyperplanes} \parencite{Scholkopf2001a}
		\item \enquote{among all hyperplanes separating the data, there exists a unique optimal hyperplane, distinguished by the maximum margin of separation between any training point and the hyperplane} \parencite{Scholkopf2001a}
		
		\item One Class SVM
			\begin{itemize}
				\item aka. novelty detection
				\item all training data is considered good
				\item decision surface is fitted as close as possible to the training data
				\item new data outside this area, surrounded by decision surfaces, is considered an outlier
				\item \enquote{[...] does not require training data to be labelled to determine a decision surface.} \parencite{Lazarevic2003} (seems to be not exactly One-Class SVM)
				\item 
			\end{itemize}
	\end{itemize}
	
	\subsection{Isolation Forest}
	\label{sec:background:network:novelty:isoforest}
	
	\begin{itemize}
		\item \enquote{explicitly isolates anomalies rather than profiles normal instances} \parencite{Liu2008}
		\item \enquote{takes advantage of two anomalies’ quantitative properties} \parencite{Liu2008}
			\subitem \enquote{they are the minority}
			\subitem \enquote{have attribute-values that are very different}
	\end{itemize}
	
	\subsection{Elliptical Envelope}
	\label{sec:background:network:novelty:envelope}
	
	\begin{itemize}
		\item tries to fit data to estimated \emph{shape}
		\item does not seem to be a good match, since it required to know the distribution of the vector fields
	\end{itemize}

	

	\subsection{Entropy based}
	%\subsection{Bayesian}
	%\subsection{Pattern Matching}
	\subsection{Autoassociative Kernel Regression (AAKR)}
		used by \textcite{Yang2006}

\section{Time-based Anomaly Detection}

\section{Prior Work and Existing Solutions}
\label{sec:background:network:priorwork}

\hint{as own chapter?}
\begin{itemize}
	\item \parencite{Yang2006}
	\item \parencite{Celeda2012}
	\item \textcite{Pan2014} BACnet
		\begin{itemize}
			\item Anonmaly detection for BACnet fire alarm systems
			\item taps BACnet traffic via IP
			\item rule based learning (RIPPER)
			\item protection against common BACnet attack vectors
				\subitem who-is/who-has network probing
				\subitem write-property (take control over device)
				\subitem InitializedRoutingTable
				\subitem reinitialize devices
				\subitem application layer DoS
				\subitem flooding of network
		\end{itemize}
	
	\item \textcite{Eskin2002} algos for unsupervised anomaly detection in IDS
	\item \textcite{Leung2005} Unsupervised anomaly detection in IDS, proposed new algo \emph{pfMAFIA}
\end{itemize}

\section{Methods of Feature Encoding}
\label{sec:background:network:features}
\label{sec:background:network:features:hashing}
\label{sec:background:network:features:onehot}

\begin{itemize}
	\item use feature direct as vector-dimension (not so good)
	\item OneHot encoding
		\subitem one vector-dimension per possible feature vector
		\subitem if feature has specific value, set dimension for this value to 1, the rest 0
		\subitem also cf. \textcite[][p. 12]{Eskin2002}, not mentioned as term, but good math-like description
		\subitem reversible
	\item Feature Hashing / Hashing Trick
		\subitem defined amount of dimensions
		\subitem feature value is hashed
		\subitem hash value is then used
		\subitem non reversible
		\subitem possibility of hash collision
		\subitem esp. when dimension count is low
		\subitem \enquote{\emph{hashing-trick:} one hashes high dimensional input vectors $x$ into a \emph{lower} dimensional feature space $\mathbb{R}^m$ with $\phi: X \rightarrow \mathbb{R}^m$} \parencite{Weinberger2009,Shi2009,Langford2007}
		\subitem \enquote{Different from random projections, the hashing-trick preserves sparsity and introduces no additional overhead to store projection matrices} \parencite{Weinberger2009}
	\item "normalize all [...] attributes to the number of standard deviations away from the mean." \parencite{Eskin2002}
		\subitem "scale based on the likelihood of the attribute values" \parencite{Eskin2002}
		
\end{itemize}